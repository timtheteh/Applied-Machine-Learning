{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Lecture 9: Classification Metrics\n",
    "-----------\n",
    "\n",
    "UBC 2022-23 W2\n",
    "\n",
    "Instructor: Amir Abdi\n",
    " - Office Hours: Mondays 5-6 (or 5-7 if student turn-out was high)\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Announcements\n",
    "\n",
    "- HW4 is due Feb 10, 11:59pm\n",
    "- Midterm Feb 15 Wednesday\n",
    "- Please make use of **OH** and **tutorials** to ask your questions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Learning outcomes \n",
    "\n",
    "From this lecture, students are expected to be able to:\n",
    "\n",
    "- Explain why accuracy is not always the best metric in ML.\n",
    "- Explain components of a **confusion matrix**. \n",
    "- Define **precision**, **recall**, and **f1-score** and use them to evaluate different classifiers. \n",
    "- Broadly explain macro-average, weighted average.\n",
    "- Interpret and use precision-recall curves. \n",
    "- Explain average precision score.\n",
    "- Interpret and use ROC curves and ROC AUC using `scikit-learn`.  \n",
    "- Identify whether there is class imbalance and whether you need to deal with it.\n",
    "- Explain and use `class_weight` to deal with data imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legends\n",
    "\n",
    "    \n",
    "| <img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f8/This_is_the_photo_of_Arthur_Samuel.jpg\" width=\"100\"> | <img src=\"http://www.cs.cmu.edu/~tom/TomHead2-6-22-22.jpg\" width=\"100\">  | <img src=\"https://upload.wikimedia.org/wikipedia/commons/4/49/John_McCarthy_Stanford.jpg\" width=\"100\"> | <img src=\"https://datascience.columbia.edu/wp-content/uploads/2020/08/Vapnik_web.png\" width=\"100\"> | <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a1/Alan_Turing_Aged_16.jpg\" width=\"100\"> | <img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1e/Yoshua_Bengio_2019_cropped.jpg\" width=\"100\"> |\n",
    "| :-----------: | :-----------: | :-----------: | :-----------: | :-----------: | :-----------: | \n",
    "| Arthur Samuel       | Tom Mitchell       |John McCarthy|  Vladimir N. Vapnik | Alan Turing | Yoshua Bengio |\n",
    "| (1901-1990)    | 1951 - Now       |  1927 – 2011 | 1936 - Now | 1912 – 1954 | 1964-Now |\n",
    "| First computer learning program | 1997 ML Texbook, CMU Prof | Co-coined term AI, Lisp,<br> Time-sharing, Garbage collection | SVM | Turing Test, Turning Machine | Turing Award<br> Father of Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation (Evaluation metrics for binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../code/.\")\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# Changing global matplotlib settings for confusion matrix.\n",
    "plt.rcParams[\"xtick.labelsize\"] = 18\n",
    "plt.rcParams[\"ytick.labelsize\"] = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Dataset for demonstration \n",
    "\n",
    "- Let's classify fraudulent and non-fraudulent transactions using Kaggle's [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cc_df = pd.read_csv(\"../data/creditcard.csv\", encoding=\"latin-1\")\n",
    "train_df, test_df = train_test_split(cc_df, test_size=0.3, random_state=111)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Good size dataset (almost a **real-world** dataset!)\n",
    "- For confidentially reasons, it only provides **transformed features with PCA** and **annonymized**, \n",
    "  - PCA is a popular dimensionality reduction technique [learn more here](https://en.wikipedia.org/wiki/Principal_component_analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We do not have categorical features. **All features are numeric**. \n",
    "- We have to be careful about the `Time` and `Amount` features. \n",
    "- We could scale `Amount`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do we want to scale **time**?\n",
    "- What does **time** tell us about the sample?\n",
    "  - Could some fradulent activities happen at the same time (temporal correlation)?\n",
    "  - Could fradulent activities today tell us something about fradulent activities in the future?\n",
    "  \n",
    "For now, in this lecture, we will drop **time**; later on, in another lecture, we will focus more on **time** as a feature in time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's separate `X` and `y` for train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train_big, y_train_big = train_df.drop(columns=[\"Class\", \"Time\"]), train_df[\"Class\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"Class\", \"Time\"]), test_df[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- It's easier to demonstrate evaluation metrics using an **explicit validation** set instead of using cross-validation. \n",
    "- And we have **enough data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/train-valid-test-split.png' width=\"1500\" height=\"1500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_big, y_train_big, test_size=0.3, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Baseline: DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Observations \n",
    "\n",
    "- `DummyClassifier` is getting **0.998 ACCURACY on validation set**!! \n",
    "- Should we be happy with this accuracy and deploy this `DummyClassifier` model for fraud detection? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What's the class distribution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We have **class imbalance**. \n",
    "  - We have **MANY non-fraud transactions** and **only a handful of fraud transactions**. \n",
    "- So in the training set, `most_frequent` strategy is labeling 199,025 (99.83%) instances correctly and only 339 (0.17%) instances incorrectly. \n",
    "- Is this what we want? \n",
    "- The \"fraud\" class is the important class that we want to spot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----------------\n",
    "Let's scale the features and try `LogisticRegression`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_valid, y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We are getting a slightly better score with logistic regression.  \n",
    "- What score should be considered an acceptable score here? \n",
    "- Are we actually spotting any \"fraud\" transactions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `.score` in **Classification** by default returns **ACCURACY** which is \n",
    "$$\\text{accuracy} = \\frac{\\text{correct predictions}}{\\text{total examples}}$$\n",
    "- Is accuracy a good metric here? \n",
    "- Is there anything more informative than accuracy that we can use here? \n",
    "\n",
    "Let's dig a little deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One way to get a better understanding of the errors is by looking at \n",
    "- false positives (type I errors), where the model incorrectly spots examples as fraud\n",
    "- false negatives (type II errors), where it's missing to spot fraud examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# --------- New Code ---------------\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    "    colorbar=False,\n",
    ");\n",
    "# --------- New Code ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare that with the Confusion Matrix of Dummy Clasifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    dummy,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    "    colorbar=False,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from plotting_functions import plot_confusion_matrix_example\n",
    "\n",
    "predictions = pipe.predict(X_valid)\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "plot_confusion_matrix_example(TN, FP, FN, TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Perfect prediction has all values down the diagonal\n",
    "- Off diagonal entries can often tell us about what is being mis-predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is \"positive\" and \"negative\"?\n",
    "\n",
    "- Two kinds of binary classification problems \n",
    "    - Distinguishing between two classes\n",
    "    - Spotting a class (spot fraud transaction, spot spam, spot disease)\n",
    "- In case of spotting problems, the thing that we are interested in spotting is considered \"positive\". \n",
    "- Above we wanted to **spot (detect) fraudulent transactions** and so **Fraudulents are \"positive\"**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can get a numpy array of confusion matrix as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = pipe.predict(X_valid)\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "print(\"Confusion matrix for fraud data set\")\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "[Read for yourself at home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confusion matrix with cross-validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- You can also calculate confusion matrix with cross-validation using the `cross_val_predict` method.  \n",
    "- But then you cannot conveniently use `plot_confusion_matrix`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "confusion_matrix(y_train, cross_val_predict(pipe, X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[end of Read for yourself at home\n",
    "\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision, recall, f1 score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We have been using `.score` to assess our models, which returns **ACCURACY** by default. \n",
    "- Accuracy is misleading when we have class imbalance.\n",
    "- We need other metrics to assess our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We'll discuss three commonly used metrics which are based on confusion matrix: \n",
    "    - recall\n",
    "    - precision\n",
    "    - f1 score \n",
    "- Later we'll talk about a few ways to address class imbalance problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "predictions = pipe_lr.predict(X_valid)\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with ACCURACY:\n",
    "\n",
    "$$ accuracy = \\frac{TP+TN}{all} = \\frac{TP+TN}{TP+TN+FP+FN} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision \n",
    "\n",
    "Among the positive examples you identified, how many were actually positive?\n",
    "\n",
    "$$ precision = \\frac{TP}{TP+FP} = \\frac{TP}{\\#detected\\_positives} $$\n",
    "\n",
    "------\n",
    "Precision captures the **Quality** of detection\n",
    "- from detected ones, how many are correct?\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TP = %0.4f, FP = %0.4f\" % (TP, FP))\n",
    "precision = TP / (TP + FP)\n",
    "print(\"Precision: %0.4f\" % (precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall \n",
    "\n",
    "Among all positive examples, how many did you identify?\n",
    "$$ recall = \\frac{TP}{TP+FN} = \\frac{TP}{\\#actual\\_positives} $$\n",
    "\n",
    "------\n",
    "Recall captures the **Quantity** of detecting positives\n",
    "- How many of the total positives have we detected?\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TP = %0.4f, FN = %0.4f\" % (TP, FN))\n",
    "recall = TP / (TP + FN)\n",
    "print(\"Recall: %0.4f\" % (recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### F1-score\n",
    "\n",
    "- F1-score combines precision and recall to give one score, which could be used in hyperparameter optimization, for instance. \n",
    "- F1-score is a **harmonic mean** of precision and recall.\n",
    "\n",
    "\n",
    "$$ f1 = 2 \\times \\frac{ precision \\times recall}{precision + recall}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"precision: %0.4f\" % (precision))\n",
    "print(\"recall: %0.4f\" % (recall))\n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "print(\"f1: %0.4f\" % (f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at all metrics at once on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate evaluation metrics by ourselves\n",
    "data = {\n",
    "    \"calculation\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"error\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1 score\": [],\n",
    "}\n",
    "data[\"calculation\"].append(\"manual\")\n",
    "data[\"accuracy\"].append((TP + TN) / (TN + FP + FN + TP))\n",
    "data[\"error\"].append((FP + FN) / (TN + FP + FN + TP))\n",
    "data[\"precision\"].append(precision)  # TP / (TP + FP)\n",
    "data[\"recall\"].append(recall)  # TP / (TP + FN)\n",
    "data[\"f1 score\"].append(f1_score)  # (2 * precision * recall) / (precision + recall)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `scikit-learn` has functions for [these metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "data[\"accuracy\"].append(accuracy_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"error\"].append(1 - accuracy_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"precision\"].append(\n",
    "    precision_score(y_valid, pipe_lr.predict(X_valid), zero_division=1)\n",
    ")\n",
    "data[\"recall\"].append(recall_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"f1 score\"].append(f1_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"calculation\"].append(\"sklearn\")\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index([\"calculation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores match, of course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `classification_report` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a convenient function called `classification_report` in `sklearn` which gives this info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_valid, pipe_lr.predict(X_valid), target_names=[\"non-fraud\", \"fraud\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️⚠️⚠️ The \"rounding to 2 precision points\" has resulted in some 1.00 in the above table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "**[study for yourself at home]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Macro average\n",
    "\n",
    "- You give equal importance to all classes and average over all classes.  \n",
    "- For instance, in the example above, recall for non-fraud is 1.0 and fraud is 0.63, and so macro average is 0.81. \n",
    "- More relevant in case of multi-class problems.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weighted average\n",
    "\n",
    "- Weighted by the number of samples in each class. \n",
    "- Divide by the total number of samples. \n",
    "\n",
    "Which one is relevant when depends upon whether you think each class should have the same weight or each sample should have the same weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 0, 1, 0]\n",
    "y_pred = [0, 0, 0, 1, 0]\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- weighted average is weighted by the proportion of examples in a particular class. So for the toy example above:\n",
    "- weighted_average precision: 3/5 * 0.75 + 2/5 * 1.00 = 0.85\n",
    "- weighted_average recall: 3/5 * 1.00 + 2/5 * 0.5 = 0.80\n",
    "- weighted_average f1-score: 3/5 * 0.86 + 2/5 * 0.67 = 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- macro average gives equal weight to both classes. So for the toy example above:\n",
    "- macro average precision: 0.5 * 0.75 + 0.5 * 1.00 =0. 875\n",
    "- macro average recall: 0.5 * 1.00 + 0.5 * 0.5 =0. 75\n",
    "- macro average f1-score: 0.5 * 0.75 + 0.5 * 1.00 =0.765"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[study for yourself at home]**\n",
    "\n",
    "----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interim summary \n",
    "\n",
    "- Accuracy is misleading when you have class imbalance. \n",
    "- A confusion matrix provides a way to break down errors made by our model. \n",
    "- We looked at three metrics based on confusion matrix: \n",
    "    - precision, recall, f1-score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Note that in **Binary Classification**, what you consider **\"positive\"** (e.g. fraud in our case) is important when calculating precision, recall, and f1-score. \n",
    "  - If you flip what is considered positive or negative, we'll end up with different TP, FP, TN, FN, and hence different precision, recall, and f1-scores. \n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evalution metrics overview  \n",
    "There is a lot of terminology here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='../img/evaluation-metrics.png' width=\"1000\" height=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `cross_validate` with different metrics\n",
    "\n",
    "We can pass different evaluation metrics with `scoring` argument of `cross_validate`.\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "# ------ New code -----------------\n",
    "scores = cross_validate(\n",
    "    pipe, X_train_big, y_train_big, return_train_score=True, scoring=[\"accuracy\", \"f1\", \"recall\", \"precision\"]\n",
    ")\n",
    "# ---------------------------------\n",
    "\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- You can also create [your own scoring function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) and pass it to `cross_validate`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### (iClicker) Exercise 9.1 \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "- (A) In medical diagnosis, false positives are likely to be more damaging than false negatives (assume \"positive\" means the person has a disease, \"negative\" means they are healthy).\n",
    "- (B) In spam classification, false positives are more damaging than false negatives (assume \"positive\" means the email is spam, \"negative\" means it's not).\n",
    "- (C) If method A gets a higher accuracy than method B, that means its precision is also higher.\n",
    "- (D) If method A gets a higher accuracy than method B, that means its recall is also higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src='img_aa/medical_fp_vs_fn.png' width=\"700\" /> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- Method A - higher accuracy but lower precision\n",
    "\n",
    "| Negative | Positive\n",
    "| -------- |:-------------:|\n",
    "| 90      | 5|\n",
    "| 5      | 0|\n",
    "\n",
    "Method B - lower accuracy but higher precision\n",
    "\n",
    "| Negative | Positive\n",
    "| -------- |:-------------:|\n",
    "| 80      | 15|\n",
    "| 0      | 5|\n",
    "\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Precision-Recall curve / ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Confusion matrix provides a detailed break down of the errors made by the model. \n",
    "- But when creating a confusion matrix, we are using \"hard\" predictions. \n",
    "- Most classifiers in `scikit-learn` provide **`predict_proba` method (or `decision_function`) which provides degree of certainty about predictions** by the classifier. \n",
    "- Can we explore the degree of uncertainty to understand and improve the model performance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's revisit the classification report on our fraud detection example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe_lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipe_lr.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**By default, predictions use the threshold of 0.5. If `predict_proba` > 0.5, predict \"fraud\" else predict \"non-fraud\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ------- New Code -------------------\n",
    "y_pred = pipe_lr.predict_proba(X_valid)[:, 1] > 0.50\n",
    "# ------------------------------------\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Suppose for your business it is more costly to miss fraudulent transactions and you want to achieve a recall of at least 75% for the \"fraud\" class. \n",
    "- One way to do this is by changing the threshold of `predict_proba`.\n",
    "    - `predict` returns 1 when `predict_proba`'s probabilities are above 0.5 for the \"fraud\" class.\n",
    "\n",
    "**Key idea: what if we threshold the probability at a smaller value so that we identify more examples as \"fraud\" examples?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's lower the threshold to **0.1**   \n",
    "In other words, predict the examples as \"fraud\" if `predict_proba` > 0.1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lower_threshold = pipe_lr.predict_proba(X_valid)[:, 1] > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, y_pred_lower_threshold, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision/Recall tradeoff \n",
    "\n",
    "- But there is a trade-off between precision and recall. \n",
    "- If you identify more things as \"fraud\", recall is going to increase but there are likely to be more false positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's sweep through different thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.0, 1.0, 0.1)\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def f(threshold):\n",
    "    preds = pipe_lr.predict_proba(X_valid)[:, 1] > threshold    \n",
    "    print(\"Threshold: \", np.round(threshold,4))\n",
    "    print(\"Precision: \", np.round(precision_score(y_valid, preds),4))\n",
    "    print(\"Recall: \", np.round(recall_score(y_valid, preds), 4))    \n",
    "    print(\"f1 score: \", np.round(f1_score(y_valid, preds), 4))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive\n",
    "\n",
    "interactive(\n",
    "    f,\n",
    "    threshold=widgets.FloatSlider(min=0, max=0.9, step=0.1, value=0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view P and R for different thresholds as a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pr_dict = {\"threshold\": [], \"precision\": [], \"recall\": [], \"f1 score\": []}\n",
    "for threshold in thresholds:\n",
    "    preds = pipe_lr.predict_proba(X_valid)[:, 1] > threshold\n",
    "    pr_dict[\"threshold\"].append(threshold)\n",
    "    pr_dict[\"precision\"].append(precision_score(y_valid, preds))\n",
    "    pr_dict[\"recall\"].append(recall_score(y_valid, preds))\n",
    "    pr_dict[\"f1 score\"].append(f1_score(y_valid, preds))\n",
    "pd.DataFrame(pr_dict)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decreasing the threshold\n",
    "\n",
    "- Decreasing the threshold means a lower bar for predicting fraud. \n",
    "    - You are willing to risk more false positives in exchange of more true positives. \n",
    "    - recall would either stay the same or go up and precision is likely to go down\n",
    "    - occasionally, precision may increase if all the new examples after decreasing the threshold are TPs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Increasing the threshold\n",
    "\n",
    "- Increasing the threshold means a higher bar for predicting fraud. \n",
    "    - recall would go down or stay the same but precision is likely to go up \n",
    "    - occasionally, precision may go down as the denominator for precision is TP+FP.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Operating point \n",
    "\n",
    "How to decide which threshold to use?\n",
    "- It is decided by the **operating point** of the business.\n",
    "- Setting a requirement on a classifier (e.g., recall of >= 0.75) is called setting the **operating point**. \n",
    "- It's usually driven by **business goals** and is useful to make **performance guarantees to customers**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision-Recall (PR) curve\n",
    "\n",
    "Often, when developing a model, it's not always clear what the operating point will be and to understand the the model better, it's informative to look at all possible thresholds and corresponding trade-offs of precision and recall in a plot.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_valid, pipe_lr.predict_proba(X_valid)[:, 1]\n",
    ")\n",
    "plt.plot(precision, recall, label=\"logistic regression: PR curve\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.plot(\n",
    "    precision_score(y_valid, pipe_lr.predict(X_valid)),\n",
    "    recall_score(y_valid, pipe_lr.predict(X_valid)),\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Each point in the curve corresponds to a possible threshold of the `predict_proba` output. \n",
    "- We can achieve a recall of 0.8 at a precision of 0.4. \n",
    "- The red dot marks the point corresponding to the threshold 0.5.\n",
    "- **The top-most right-most point in this PR diagram would be a perfect classifier (precision = recall = 1).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The threshold is not shown here, but it's going from 0 (upper-left) to 1 (lower right).\n",
    "- At a threshold of 0 (upper left), we are classifying everything  as \"fraud\".\n",
    "- Raising the threshold increases the precision but at the expense of lowering the recall. \n",
    "- At the extreme right, where the threshold is 1, we get into the situation where all the examples classified as \"fraud\" are actually \"fraud\"; we have no false positives. \n",
    "- Here we have a high precision but lower recall. \n",
    "- Usually the goal is to **keep recall high as precision goes up**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A few comments on PR curve\n",
    "\n",
    "- Different classifiers might work well in different parts of the curve, i.e., at different operating points.   \n",
    "- We can compare PR curves of different classifiers to understand these differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Average Precision (AP) score \n",
    "\n",
    "- Often it's useful to have one number summarizing the PR plot (e.g., in hyperparameter optimization)\n",
    "- One way to do this is by **computing the area under the PR curve**. \n",
    "- This is called **average precision** (AP score)\n",
    "- AP score has a value between 0 (worst) and 1 (best). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "y_predict = pipe_lr.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "#  -------- New  Code --------------\n",
    "ap_lr = average_precision_score(y_valid, y_predict)\n",
    "# ----------------------------------\n",
    "print(\"Average precision of logistic regression: {:.3f}\".format(ap_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the following handy function of `sklearn` to get the PR curve and the corresponding AP score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "#  -------- New  Code --------------\n",
    "PrecisionRecallDisplay.from_estimator(pipe_lr, X_valid, y_valid);\n",
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AP vs. F1-score\n",
    "\n",
    "It is very important to note this distinction:\n",
    "\n",
    "- F1 score is for a given threshold and measures the quality of `predict`.\n",
    "- AP score is a summary across thresholds and measures the quality of `predict_proba`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Remember to pick the desired threshold based on the results on the validation set and **NOT** on the test set.\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br>\n",
    "For classification problems with **imbalanced classes**, using AP score is more meaningful than using accuracy. \n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A few comments on PR curve\n",
    "\n",
    "- Different classifiers might work well in different parts of the curve, i.e., at different operating points.   \n",
    "- We can compare PR curves of different classifiers to understand these differences. \n",
    "- Let's create PR curves for SVC and Logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "pipe_svc = make_pipeline(StandardScaler(), SVC())\n",
    "pipe_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get precision and recall for different thresholds? \n",
    "- Use the function `precision_recall_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_lr, recall_lr, thresholds_lr = precision_recall_curve(\n",
    "    y_valid, pipe_lr.predict_proba(X_valid)[:, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_lr.shape, precision_lr.shape, recall_lr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "How many thresholds? \n",
    "- The number of **unique `predict_proba` scores** in our dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(pipe_lr.predict_proba(X_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each threshold, precision and recall is calculated.  \n",
    "- The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About SVC and confidence of predictions:\n",
    "- SVC doesn't have `predict_proba`. Instead it has something called `decision_function`. \n",
    "- The index of the threshold that is closest to 0 of decision function is the default threshold in SVC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "precision_lr, recall_lr, thresholds_lr = precision_recall_curve(\n",
    "    y_valid, pipe_lr.predict_proba(X_valid)[:, 1]\n",
    ")\n",
    "precision_svc, recall_svc, thresholds_svc = precision_recall_curve(\n",
    "    y_valid, pipe_svc.decision_function(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Finding the threhold indices which are close to zero (default threshold) in LR and SVC\n",
    "close_default_lr = np.argmin(np.abs(thresholds_lr - 0.5))\n",
    "close_zero_svm = np.argmin(np.abs(thresholds_svc))\n",
    "\n",
    "# A bit of plotting\n",
    "plt.plot(precision_svc, recall_svc, label=\"svc\")\n",
    "plt.plot(precision_lr, recall_lr, label=\"logistic regression\")\n",
    "plt.plot(\n",
    "    precision_svc[close_zero_svm],\n",
    "    recall_svc[close_zero_svm],\n",
    "    \"o\",\n",
    "    markersize=10,\n",
    "    label=\"default threshold svc\",\n",
    "    c=\"b\",\n",
    ")\n",
    "plt.plot(\n",
    "    precision_lr[close_default_lr],\n",
    "    recall_lr[close_default_lr],\n",
    "    \"*\",\n",
    "    markersize=10,\n",
    "    label=\"default threshold logistic regression\",\n",
    "    c=\"r\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "**[Study on your own]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svc_preds = pipe_svc.predict(X_valid)\n",
    "lr_preds = pipe_lr.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"f1_score of logistic regression: {:.3f}\".format(f1_score(y_valid, lr_preds)))\n",
    "print(\"f1_score of svc: {:.3f}\".format(f1_score(y_valid, svc_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ap_lr = average_precision_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "ap_svc = average_precision_score(y_valid, pipe_svc.decision_function(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average precision of logistic regression: {:.3f}\".format(ap_lr))\n",
    "print(\"Average precision of SVC: {:.3f}\".format(ap_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Comparing the precision-recall curves provide us a detail insight compared to f1 score.\n",
    "- For example, F1 scores for SVC and logistic regressions are pretty similar. In fact, f1 score of logistic regression is a tiny bit better. \n",
    "- But when we look at the PR curve, we see that SVC is doing better than logistic regression for most of the other thresholds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[end of Study on your own]**\n",
    "\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Receiver Operating Characteristic (ROC) curve \n",
    "\n",
    "- Another commonly used tool to analyze the behavior of classifiers at different thresholds.  \n",
    "- Similar to PR curve, it considers all possible thresholds for a given classifier given by `predict_proba` \n",
    "  - but instead of precision and recall it plots **false positive rate (FPR)** and **true positive rate (TPR or recall)**.\n",
    "$$ TPR = \\text{recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "$$FPR  = \\frac{FP}{FP + TN}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "\n",
    "default_threshold = np.argmin(np.abs(thresholds - 0.5))\n",
    "\n",
    "plt.plot(\n",
    "    fpr[default_threshold],\n",
    "    tpr[default_threshold],\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The **ideal** curve is close to the **top-left**\n",
    "    - Ideally, you want a classifier with high recall while keeping low false positive rate.  \n",
    "- The red dot corresponds to the threshold of 0.5, which is used by predict.\n",
    "- **In this example**, we see that compared to the default threshold, **we can achieve a better recall of around 0.8 without increasing FPR**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "**[Study on your own]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare ROC curve of different classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "\n",
    "fpr_svc, tpr_svc, thresholds_svc = roc_curve(\n",
    "    y_valid, pipe_svc.decision_function(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_default_lr = np.argmin(np.abs(thresholds_lr - 0.5))\n",
    "close_zero_svm = np.argmin(np.abs(thresholds_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr_svc, tpr_svc, label=\"svc\")\n",
    "plt.plot(fpr_lr, tpr_lr, label=\"logistic regression\")\n",
    "plt.plot(\n",
    "    fpr_svc[close_zero_svm],\n",
    "    tpr_svc[close_zero_svm],\n",
    "    \"o\",\n",
    "    markersize=10,\n",
    "    label=\"default threshold svc\",\n",
    "    c=\"b\",\n",
    ")\n",
    "plt.plot(\n",
    "    fpr_lr[close_default_lr],\n",
    "    tpr_lr[close_default_lr],\n",
    "    \"*\",\n",
    "    markersize=10,\n",
    "    label=\"default threshold logistic regression\",\n",
    "    c=\"r\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate (Recall)\")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[end of Study on your own]**\n",
    "\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Area under the curve (AUC)\n",
    "\n",
    "**AUC = Area under the ROC curve**\n",
    "\n",
    "AUC provides a single meaningful number for the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_lr = roc_auc_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "roc_svc = roc_auc_score(y_valid, pipe_svc.decision_function(X_valid))\n",
    "print(\"AUC for LR: {:.3f}\".format(roc_lr))\n",
    "print(\"AUC for SVC: {:.3f}\".format(roc_svc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **AUC of 0.5 means random chance**. \n",
    "  - When AUC is approximately 0.5, the model has no discrimination capacity to distinguish between positive class and negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC can be interpreted as evaluating the **ranking** of positive examples.\n",
    "- What's the probability that a randomly picked positive point has a higher score according to the classifier than a randomly picked point from the negative class. \n",
    "- AUC of 1.0 means **all positive points have a higher score than all negative points**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `PrecisionRecallCurveDisplay`, there is a `RocCurveDisplay` function in sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay.from_estimator(pipe_lr, X_valid, y_valid);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's look at all the scores at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = [\"accuracy\", \"f1\", \"recall\", \"precision\", \"roc_auc\", \"average_precision\"]\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "scores = cross_validate(pipe, X_train_big, y_train_big, scoring=scoring)\n",
    "pd.DataFrame(scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br><br>\n",
    "Check out [these visualization](https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation) on ROC and AUC.  \n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Precision-Recall (PR) Curve --> AP \n",
    "- Receiver Operating Characteristic (ROC) curve --> AUC\n",
    "- ROC curves should be used when there are roughly equal numbers of observations for each class.\n",
    "- Precision-Recall curves should be used when there is a moderate to large class imbalance.\n",
    "\n",
    "Learn more here: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dealing with class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class imbalance in training sets\n",
    "\n",
    "- This typically refers to having many more examples of one class than another in one's training set.\n",
    "- Real world data is often imbalanced. \n",
    "    - Our Credit Card Fraud dataset is imbalanced.\n",
    "    - Ad clicking data is usually drastically imbalanced. (Only around ~0.01% ads are clicked.)\n",
    "    - Spam classification datasets are also usually imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Addressing class imbalance\n",
    "A very important question to ask yourself: \"**Why** do I have a class imbalance?\"\n",
    "\n",
    "- Is it because one class is **naturally more rare** than the other?\n",
    "- Is it because of my **data collection** methods?\n",
    "  \n",
    "In some cases, it may be fine to just ignore the class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which type of error is more important? \n",
    "\n",
    "- False positives (FPs) and false negatives (FNs) have quite different real-world consequences. \n",
    "- In PR curve and ROC curve, we saw how changing the prediction threshold can change FPs and FNs. \n",
    "- We can then pick the threshold that's appropriate for our problem. \n",
    "- Example: if we want high recall, we may use a lower threshold (e.g., a threshold of 0.1). We'll then catch more fraudulent transactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred = pipe_lr.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And changing threshold to 0.10 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipe_lr.predict_proba(X_valid)[:, 1] > 0.10\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling imbalance\n",
    "\n",
    "Can we change the model itself rather than changing the threshold so that it takes into account the errors that are important to us?\n",
    "\n",
    "There are two common approaches for this: \n",
    "- **Changing the data (optional)** (not covered here)\n",
    "   - Undersampling\n",
    "   - Oversampling \n",
    "       - Random oversampling\n",
    "       - SMOTE \n",
    "- **Changing the training procedure** \n",
    "    - `class_weight`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Changing the training procedure \n",
    "\n",
    "- Most of `sklearn` classifiers have a parameter called `class_weight`.\n",
    "- This allows you to specify that one class is more important than another.\n",
    "  - For example, maybe a false negative is 10x more problematic than a false positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: `class_weight` parameter of `sklearn LogisticRegression` \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "> class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, **class_weight=None**, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "> class_weight: dict or 'balanced', default=None\n",
    "\n",
    "> Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\"\n",
    "HTML(\"<iframe src=%s width=1000 height=650></iframe>\" % url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Let's train with default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe_lr,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Let's set \"fraud\" class a weight of 10.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr_weight = make_pipeline(\n",
    "    StandardScaler(), \n",
    "    LogisticRegression(\n",
    "        max_iter=500, \n",
    "        class_weight={0:1, 1: 10}  # ---> notice the weights\n",
    "    )\n",
    ")\n",
    "pipe_lr_weight.fit(X_train, y_train)\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe_lr_weight,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Notice we've **reduced false negatives** and predicted more Fraud this time.\n",
    "- This was equivalent to saying give 10x more \"importance\" to fraud class. \n",
    "- Note that as a consequence we are also **increasing false positives**.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `class_weight=\"balanced\"`\n",
    "- A useful setting is `class_weight=\"balanced\"`.\n",
    "- This sets the weights so that the classes are \"equal\".\n",
    "\n",
    "> class_weight: dict, ‘balanced’ or None\n",
    "\n",
    "> The “balanced” mode uses the values of y to automatically **adjust weights inversely proportional** to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n",
    "\n",
    "> sklearn.utils.class_weight.compute_class_weight(class_weight, classes, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr_balanced = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(max_iter=500, class_weight=\"balanced\")\n",
    ")\n",
    "pipe_lr_balanced.fit(X_train, y_train)\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe_lr_balanced,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reduced false negatives but we have many more false positives now ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Are we doing better with `class_weight=\"balanced\"`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dict = {}\n",
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500))\n",
    "scoring = [\"accuracy\", \"f1\", \"recall\", \"precision\", \"roc_auc\", \"average_precision\"]\n",
    "orig_scores = cross_validate(pipe_lr, X_train_big, y_train_big, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr_balanced = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(max_iter=500, class_weight=\"balanced\")\n",
    ")\n",
    "scoring = [\"accuracy\", \"f1\", \"recall\", \"precision\", \"roc_auc\", \"average_precision\"]\n",
    "bal_scores = cross_validate(pipe_lr_balanced, X_train_big, y_train_big, scoring=scoring)\n",
    "comp_dict = {\n",
    "    \"Original\": pd.DataFrame(orig_scores).mean().tolist(),\n",
    "    \"class_weight='balanced'\": pd.DataFrame(bal_scores).mean().tolist(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(comp_dict, index=bal_scores.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall is much better but precision has dropped a lot; we have many false positives. \n",
    "- You could also optimize `class_weight` using hyperparameter optimization for your specific problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Changing the class weight will **generally reduce accuracy**.\n",
    "  - The original model was trying to maximize accuracy.\n",
    "  - Now you're telling it to do something different.\n",
    "- But that can be fine, **accuracy isn't the only metric that matters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----------\n",
    "**[Reminder: Covered before]**\n",
    "**[Re-study on your own]**\n",
    "\n",
    "### Stratified Sampling when splitting data into Train/Valid/Test\n",
    "\n",
    "- A similar idea of \"balancing\" classes can be applied to data splits.\n",
    "- We have the same option in `train_test_split` with the `stratify` argument. \n",
    "- By default it splits the data so that if we have 10% negative examples in total, then each split will have 10% negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stratified-sampling](https://cdn.scribbr.com/wp-content/uploads/2020/09/stratified-sample-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- If you are carrying out cross validation using `cross_validate`, by default it uses [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html). From the documentation: \n",
    "\n",
    "> This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n",
    "\n",
    "- In other words, if we have 10% negative examples in total, then each fold will have 10% negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**[End of Re-study on your own]**\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What did we learn today? \n",
    "\n",
    "- A number of possible ways to evaluate Classifiers\n",
    "    - Choose the evaluation metric that makes most sense in your context or which is most common in your discipline  \n",
    "- Two kinds of binary classification problems \n",
    "    - Distinguishing between two classes (e.g., dogs vs. cats)\n",
    "    - Spotting a class (e.g., spot fraud transaction, spot spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Precision, recall, f1-score are useful when dealing with spotting problems. \n",
    "- The thing that we are interested in spotting is considered \"positive\".   \n",
    "- Do you need to deal with class imbalance in the given problem? \n",
    "- Methods to deal with class imbalance \n",
    "    - Changing the training procedure \n",
    "        - `class_weight`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relevant papers and resources \n",
    "\n",
    "- [The Relationship Between Precision-Recall and ROC Curves](https://www.biostat.wisc.edu/~page/rocpr.pdf)\n",
    "- [Article claiming that PR curve are better than ROC for imbalanced datasets](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432)\n",
    "- [Precision-Recall-Gain Curves: PR Analysis Done Right](https://papers.nips.cc/paper/2015/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf)\n",
    "- [ROC animation](https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation)\n",
    "- [Generalization in Adaptive Data Analysis and Holdout Reuse](https://arxiv.org/pdf/1506.02629.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/eva-seeyou.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
