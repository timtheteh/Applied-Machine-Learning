{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "Lecture 8: Hyperparameter Optimization and Optimization Bias\n",
    "------------\n",
    "\n",
    "UBC 2022-23 W2\n",
    "\n",
    "Instructor: Amir Abdi\n",
    " - Office Hours: Mondays 5-6 (or 5-7 if student turn-out was high)\n",
    "\n",
    "<br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please share your Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img_aa/qr-code-feedback-Amir.png\" height=\"300\" width=\"300\"> \n",
    "\n",
    "https://forms.gle/NLKCe3BrLC6tLpkE8\n",
    "\n",
    "It's 100% **anonymous**\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "There is a lot of contents we want to cover in each session; but, time is limited.\n",
    "\n",
    "I jump over some cells in the Notebook; and **I trust that you will study them at home**.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "**Reminder:** \n",
    "- Contents in the notebooks are for you to study; we won't be going through every cell in the 80-minute class. \n",
    "- Goal of the instructor sessions is to **learn Machine Learning concepts**.\n",
    "- Goal of HW and Tutorials is to **learn to Apply those concepts (i.e., to code)**\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Announcements, and LO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../code/.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Announcements\n",
    "\n",
    "- HW3 was due last night. \n",
    "- HW4 is released.\n",
    "- Midterm is coming up: **Feb 15 Wednesday**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning outcomes \n",
    "\n",
    "From this lecture, you will be able to \n",
    "\n",
    "- explain the need for **hyperparameter optimization**\n",
    "- carry out hyperparameter optimization using `sklearn`'s `GridSearchCV` and `RandomizedSearchCV` \n",
    "- explain different hyperparameters of `GridSearchCV`\n",
    "- explain the importance of selecting a good range for the values. \n",
    "- explain optimization bias\n",
    "- identify and reason when to trust and not trust reported accuracies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameter optimization motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "- Remember that the fundamental goal of supervised machine learning is to generalize beyond what we see in the training examples. \n",
    "- We have been using data splitting and cross-validation to provide a framework to approximate generalization error.  \n",
    "- With this framework, we can improve the model's generalization performance by tuning model hyperparameters using cross-validation on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameters: the problem\n",
    "\n",
    "- In order to improve the generalization performance, finding the best values for the important hyperparameters of a model is necessary. \n",
    "- Picking good hyperparameters is important because if we don't do it, we might end up with an underfit or overfit model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "**Every decision you make in Machine Learning is a Hyper-Parameter (HParam):**\n",
    "- How to clean data\n",
    "- Which features to include \n",
    "- How to scale data\n",
    "- Which ML model to choose\n",
    "- Which parameters to decide for the model \n",
    "  - max-depth in tree, \n",
    "  - C value in SVM, \n",
    "  - gamma value in RBF, \n",
    "  - alpha value in Ridge Regression, \n",
    "  - C value in Logistic Regression\n",
    "- ...\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some ways to pick hyperparameters:\n",
    "- Expert knowledge \n",
    "- Heuristics\n",
    "- Search hyper-parameter space\n",
    "- Data-driven or automated optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Manual hyperparameter optimization\n",
    "\n",
    "- Advantage: we may have some intuition about what might work.\n",
    "  - E.g. if I'm massively overfitting, try decreasing `max_depth` or `C`.\n",
    "- Disadvantages\n",
    "    - it takes a lot of work\n",
    "    - not reproducible, **not scalable**\n",
    "    - in very complicated cases, our intuition might be worse than a data-driven approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Automated hyperparameter optimization \n",
    "\n",
    "- Formulate the hyperparamter optimization as a one big **search problem**. \n",
    "- Often we have many hyperparameters of different types: Categorical, integer, and continuous.\n",
    "- Often, the search space is quite big and systematic search for optimal values is infeasible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In homework assignments, we have been carrying out hyperparameter search by exhaustively trying different possible combinations of the hyperparameters of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_grid_search_overview()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at an example of tuning `max_depth` of the `DecisionTreeClassifier` on the Spotify dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spotify_df = pd.read_csv(\"../data/spotify.csv\", index_col=0)\n",
    "X_spotify = spotify_df.drop(columns=[\"target\", \"song_title\", \"artist\"])\n",
    "y_spotify = spotify_df[\"target\"]\n",
    "X_spotify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_spotify, y_spotify, test_size=0.2, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try **DecisionTreeClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "\n",
    "# --------- ATTENTION HERE ----------------\n",
    "param_grid = {\"max_depth\": np.arange(1, 20, 2)}\n",
    "# --------- ATTENTION HERE ----------------\n",
    "\n",
    "results_dict = {\"max_depth\": [], \"mean_cv_score\": []}\n",
    "for depth in param_grid[\"max_depth\"]:  # for each combination of parameters, train an SVC\n",
    "    dt = DecisionTreeClassifier(max_depth=depth)\n",
    "    scores = cross_val_score(dt, X_train, y_train)  # perform cross-validation\n",
    "    mean_score = np.mean(scores)  # compute mean cross-validation accuracy\n",
    "\n",
    "    if (mean_score > best_score):  # if we got a better score, store the score and parameters\n",
    "        best_score = mean_score\n",
    "        best_params = {\"max_depth\": depth}\n",
    "\n",
    "    results_dict[\"max_depth\"].append(depth)\n",
    "    results_dict[\"mean_cv_score\"].append(mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_dict).sort_values(by=\"mean_cv_score\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(best_params, ', score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Try **SVM RBF** and tuning `C` and `gamma` on the same dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_svm = make_pipeline(StandardScaler(), SVC())  # We need scaling for SVM RBF\n",
    "pipe_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try cross-validation with default hyperparameters of SVC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(pipe_svm, X_train, y_train, return_train_score=True)\n",
    "print('validation score:', pd.DataFrame(scores).mean()['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now let's try exhaustive hyperparameter search using for loops. \n",
    "\n",
    "This is what we have been doing for this (**pseudocode warning**):\n",
    "\n",
    "```\n",
    "for gamma in [0.01, 1, 10, 100]: # for some values of gamma\n",
    "    for C in [0.01, 1, 10, 100]: # for some values of C\n",
    "        for fold in folds:\n",
    "            fit in training portion with the given C\n",
    "            score on validation portion\n",
    "        compute average score\n",
    "        \n",
    "pick hyperparameter values which yield with best average score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "\n",
    "# --------- ATTENTION HERE ----------------\n",
    "param_grid = {\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    \"gamma\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "# --------- ATTENTION HERE ----------------\n",
    "\n",
    "results_dict = {\"C\": [], \"gamma\": [], \"mean_cv_score\": []}\n",
    "\n",
    "for gamma in param_grid[\"gamma\"]:\n",
    "    for C in param_grid[\"C\"]:  # for each combination of parameters, train an SVC\n",
    "\n",
    "        pipe_svm = make_pipeline(StandardScaler(), SVC(gamma=gamma, C=C))\n",
    "        scores = cross_val_score(pipe_svm, X_train, y_train, cv=5)  # perform cross-validation\n",
    "        mean_score = np.mean(scores)  # compute mean cross-validation accuracy\n",
    "        if (mean_score > best_score):  # if we got a better score, store the score and parameters\n",
    "            best_score = mean_score\n",
    "            best_parameters = {\"C\": C, \"gamma\": gamma}\n",
    "            \n",
    "        results_dict[\"C\"].append(C)\n",
    "        results_dict[\"gamma\"].append(gamma)\n",
    "        results_dict[\"mean_cv_score\"].append(mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(best_parameters, \", best_score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=\"mean_cv_score\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the mean cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scores = np.array(df.mean_cv_score).reshape(6, 6)\n",
    "\n",
    "mglearn.tools.heatmap(\n",
    "    scores,\n",
    "    xlabel=\"C\",\n",
    "    xticklabels=param_grid[\"C\"],\n",
    "    ylabel=\"gamma\",\n",
    "    yticklabels=param_grid[\"gamma\"],\n",
    "    cmap=\"viridis\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We have 6 possible values for `C` and 6 possible values for `gamma`. \n",
    "- In 5-fold cross-validation, for each combination of parameter values, five accuracies are computed.\n",
    "- So to evaluate the accuracy of the SVM using 6 values of `C` and 6 values of `gamma` using five-fold cross-validation, **we need to train 36 * 5 = 180 models**! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once we have optimized hyperparameters, we retrain a model on the full training set with these optimized hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm = make_pipeline(StandardScaler(), SVC(**best_parameters))\n",
    "pipe_svm.fit(\n",
    "    X_train, y_train\n",
    ")  # Retrain a model with optimized hyperparameters on the combined training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally evaluate the performance of this model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm.score(X_test, y_test)  # Final evaluation on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This process is so common that there are some standard methods in `scikit-learn` where we can carry out all of this in a more compact way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_grid_search_overview()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this lecture we are going to talk about two such most commonly used automated optimizations methods from `scikit-learn`. \n",
    "\n",
    "- Exhaustive grid search: [`sklearn.model_selection.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "- Randomized search: [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "The \"CV\" stands for cross-validation; these methods have built-in cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exhaustive grid search: [`sklearn.model_selection.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- For `GridSearchCV` we need\n",
    "    - an instantiated model or a pipeline\n",
    "    - a parameter grid: A user specifies a set of values for each hyperparameter. \n",
    "    - other optional arguments \n",
    "\n",
    "The method considers product of the sets and evaluates each combination one by one.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_svm = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "# ---------- ATTENTION TO __ -----------\n",
    "param_grid = {\n",
    "    \"svc__gamma\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n",
    "    \"svc__C\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n",
    "}\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "# ---------- NEW CODE -----------\n",
    "grid_search = GridSearchCV(\n",
    "    pipe_svm, param_grid, cv=5, n_jobs=-1, return_train_score=True\n",
    ")\n",
    "# ------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_jobs=-1` PARAMETER --> be patient :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The `GridSearchCV` object above behaves like a classifier. We can call `fit`, `predict` or `score` on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# -------- NEW CODE ---------------\n",
    "grid_search.fit(X_train, y_train) # all the work is done here\n",
    "# -------- NEW CODE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fitting the `GridSearchCV` object \n",
    "- Searches for the best hyperparameter values\n",
    "- You can access the best score and the best hyperparameters using `best_score_` and `best_params_` attributes, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- It is often helpful to visualize results of all cross-validation experiments. \n",
    "- You can access this information using `cv_results_` attribute of a fitted `GridSearchCV` object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's only sort the dataframe on `rank_test_score` and look at the **most relevant rows**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)[\n",
    "    [\n",
    "        \"mean_test_score\",\n",
    "        \"param_svc__gamma\",\n",
    "        \"param_svc__C\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_score\",\n",
    "    ]\n",
    "].set_index(\"rank_test_score\").sort_index().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br><br><br><br><br>\n",
    "- Other than searching for best hyperparameter values, `GridSearchCV` also fits a new model on the whole training set with the parameters that yielded the best results. \n",
    "- So we can conveniently call `score` on the test set with a fitted `GridSearchCV` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for the class:** Why are `best_score_` and the score above different? \n",
    "<br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Same code copied here to cover 3 concepts: n_jobs, __, and exponential values\n",
    "param_grid = {\n",
    "    \"svc__gamma\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n",
    "    \"svc__C\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    pipe_svm, param_grid, cv=5, n_jobs=-1, return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `n_jobs=-1`\n",
    "- Note the `n_jobs=-1` above.\n",
    "- Hyperparameter optimization can be done **_in parallel_** for each of the configurations (utilizing multi-cores of CPU)\n",
    "- This is very useful when scaling up to large numbers of machines in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The `__` syntax \n",
    "\n",
    "- Above: we have a nesting of transformers.\n",
    "- We can access the parameters of the \"inner\" objects by using __ to go \"deeper\":\n",
    "- `svc__gamma`: the `gamma` of the `svc` of the pipeline\n",
    "- `svc__C`: the `C` of the `svc` of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Values of the Grid\n",
    "\n",
    "- Note the **exponential increments** for `C` and `gamma`. \n",
    "- This is quite common.\n",
    "- There is no point trying $C=\\{1,2,3\\ldots,100\\}$ because $C=1,2,3$ are too similar to each other.\n",
    "- Often we're trying to find an order of magnitude, e.g. $C=\\{0.01,0.1,1,10,100\\}$. \n",
    "- We can also write that as $C=\\{10^{-2},10^{-1},10^0,10^1,10^2\\}$. \n",
    "- Or, in other words, $C$ values to try are $10^n$ for $n=-2,-1,0,1,2$ which is basically what we have above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing the parameter grid as a heatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_display_heatmap(param_grid, pipe, X_train, y_train, reshape=(6, 6)):\n",
    "\n",
    "    # Define grid and Fit\n",
    "    grid_search = GridSearchCV(\n",
    "        pipe, param_grid, cv=5, n_jobs=-1, return_train_score=True\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "        \n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    scores = np.array(results.mean_test_score).reshape(reshape)\n",
    "\n",
    "    # plot the mean cross-validation scores\n",
    "    mglearn.tools.heatmap(\n",
    "        scores,\n",
    "        xlabel=\"gamma\",\n",
    "        xticklabels=param_grid[\"svc__gamma\"],\n",
    "        ylabel=\"C\",\n",
    "        yticklabels=param_grid[\"svc__C\"],\n",
    "        cmap=\"viridis\",\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Note that the range we pick for the parameters play an important role in hyperparameter optimization. \n",
    "- For example, consider the following grid and the corresponding results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "    \"svc__gamma\": 10.0**np.arange(-3, 3, 1), \n",
    "    \"svc__C\": 10.0**np.arange(-3, 3, 1)\n",
    "}\n",
    "fit_and_display_heatmap(param_grid1, pipe_svm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Each point in the heat map corresponds to one run of cross-validation, with a particular setting\n",
    "- Colour encodes cross-validation accuracy. \n",
    "    - Lighter colour means high accuracy\n",
    "    - Darker colour means low accuracy\n",
    "- SVC is quite sensitive to hyperparameter settings.\n",
    "- Adjusting hyperparameters can change the accuracy from 0.51 to 0.74! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "**[study on your own]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bad range for hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(1, 2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(1, 2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid2 = {\"svc__gamma\": np.round(np.logspace(1, 2, 6), 1), \"svc__C\": np.linspace(1, 2, 6)}\n",
    "fit_and_display_heatmap(param_grid2, pipe_svm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different range for hyperparameters yields better results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-3, 2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(1, 2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid3 = {\"svc__gamma\": np.logspace(-3, 2, 6), \"svc__C\": np.linspace(1, 2, 6)}\n",
    "\n",
    "fit_and_display_heatmap(param_grid3, pipe_svm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It seems like we are getting even better cross-validation results with `C` = 2.0 and `gamma` = 0.1 \n",
    "\n",
    "How about exploring different values of `C` close to 2.0? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid4 = {\"svc__gamma\": np.logspace(-3, 2, 6), \"svc__C\": np.linspace(2, 3, 6)}\n",
    "\n",
    "fit_and_display_heatmap(param_grid4, pipe_svm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That's good! We are finding some more options for `C` where the accuracy is 0.75. \n",
    "The tricky part is we do not know in advance what range of hyperparameters might work the best for the given problem, model, and the dataset.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "`GridSearchCV` allows the param_grid to be a list of dictionaries. \n",
    "\n",
    "Each model has its own hyper-parameters:\n",
    "- in `SVC` with **RBF** kernel: `C` and `gamma` are applicable \n",
    "- in `SVC` with **linear** kernel: `C` is applicable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[end of study on your own]**\n",
    "\n",
    "---------------\n",
    "\n",
    "<br><br><br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems with exhaustive grid search \n",
    "\n",
    "- Required number of models to evaluate **grows exponentially** with the dimensionally of the configuration space. \n",
    "- Example: Suppose you have\n",
    "    - 5 hyperparameters \n",
    "    - 10 different values for each hyperparameter\n",
    "    - You'll be evaluating $10^5=100,000$ models! That is you'll be calling `cross_validate` 100,000 times!\n",
    "- Exhaustive search becomes **infeasible (intractable)** fairly quickly. \n",
    "\n",
    "![grid-search](https://www.yourdatateacher.com/wp-content/uploads/2021/03/image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, there is a **another important problem with Grid Search**:\n",
    "\n",
    "- Waste of compute in retrying the **same** Hyper-Param values multiple times\n",
    "  - Example: in the following figure, values of C=0.001 and C=0.01 are not producing good models; but, we keep on retrying them and wasting compute.\n",
    "- Adding **HParams that do not influence the performance** adversely impacts the **efficiency** (more wasted compute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "    \"svc__gamma\": 10.0**np.arange(-3, 1, 1), \n",
    "    \"svc__C\": 10.0**np.arange(-3, 3, 1)\n",
    "}\n",
    "fit_and_display_heatmap(param_grid1, pipe_svm, X_train, y_train, (6, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Randomized hyperparameter search\n",
    "\n",
    "- Randomized hyperparameter optimization \n",
    "    - [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "- Samples configurations at random until certain budget (e.g., time) is exhausted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"svc__gamma\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n",
    "    \"svc__C\": np.linspace(2, 3, 6),\n",
    "}\n",
    "\n",
    "print(\"Grid size: %d\" % (np.prod(list(map(len, param_grid.values())))))\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# ---------- New Code ----------------\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe_svm, param_distributions=param_grid, n_jobs=-1, n_iter=20, cv=5, random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train);\n",
    "# ---------- New Code ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the new parameter: **n_iter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`n_iter`\n",
    "\n",
    "- Note the `n_iter`, we didn't need this for `GridSearchCV`.\n",
    "- Larger `n_iter` will take longer but it'll do more searching.\n",
    "  - Remember you still need to multiply by number of folds!\n",
    "- I have set the `random_state` for **reproducibility of the HParam optimization experiment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(random_search.cv_results_)[\n",
    "    [\n",
    "        \"mean_test_score\",\n",
    "        \"param_svc__gamma\",\n",
    "        \"param_svc__C\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_score\",\n",
    "    ]\n",
    "].set_index(\"rank_test_score\").sort_index().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "**Reminder**:\n",
    "\n",
    "**An important problem with Grid Search**: Waste of compute in retrying the **same** Hyper-Param values multiple times\n",
    "\n",
    "**Question**: Did we solve the above problem in our `RandomizedSearchCV` solution?  \n",
    "Answer: ????\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability distributions for Random Search (instead of a predefined list!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another thing we can do is give probability distributions to draw from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon, lognorm, loguniform, randint, uniform, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# -------- New Code -----------\n",
    "y = uniform.rvs(0, 5, 10000) # --> sampling 10,000 random numbers, from a uniform ditribution, between 0 and 5\n",
    "# -------- New Code -----------\n",
    "\n",
    "bin = np.arange(-3,8,0.1)  \n",
    "plt.hist(y, bins=bin, edgecolor='blue') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- New Code -----------\n",
    "y = norm.rvs(3, 2, 10000) # --> sampling 10,000 random numbers, from a normal distribution, with mean=3 and std=2\n",
    "# -------- New Code -----------\n",
    "\n",
    "#creating bin\n",
    "bin = np.arange(-4,10,0.1)  \n",
    "plt.hist(y, bins=bin, edgecolor='blue') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- New Code -----------\n",
    "y = expon.rvs(5, 2, 10000) # --> sampling 10,000 random numbers, from a exponential distribution, at location=5 and scale=2\n",
    "# -------- New Code -----------\n",
    "\n",
    "#creating bin\n",
    "bin = np.arange(-1,10,0.1)  \n",
    "\n",
    "plt.hist(y, bins=bin, edgecolor='blue') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- New Code -----------\n",
    "y = loguniform.rvs(0.001, 100, scale=1000, size=10000) # --> sampling 10,000 random numbers, from a loguniform distribution\n",
    "# -------- New Code -----------\n",
    "\n",
    "#creating bin\n",
    "bin = np.arange(0.001,100,1)  \n",
    "\n",
    "plt.hist(y, bins=bin, edgecolor='blue') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing uniform and loguniform as parameters to the RandomSearch to sample from\n",
    "param_dist = {\n",
    "    \"svc__C\": loguniform(0.001, 100),\n",
    "    \"svc__gamma\": loguniform(0.001, 100),\n",
    "}\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe_svm, param_dist, n_iter=100, verbose=1, n_jobs=-1, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(random_search.cv_results_)[\n",
    "    [\n",
    "        \"mean_test_score\",\n",
    "        \"param_svc__gamma\",\n",
    "        \"param_svc__C\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_score\",\n",
    "    ]\n",
    "].set_index(\"rank_test_score\").sort_index().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- This is a bit fancy. What's nice is that you can have it concentrate more on certain values by setting the distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Advantages of `RandomizedSearchCV`\n",
    "\n",
    "- Faster compared to `GridSearchCV`.\n",
    "- Adding parameters that do not influence the performance does not harm efficiency.\n",
    "- It is recommended to use `RandomizedSearchCV` rather than `GridSearchCV`.    \n",
    "\n",
    "Check the 2012 paper from **Yoshua Bengio*** on [Random Search for Hyper-Parameter Optimization](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\n",
    "\n",
    "-----------\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1e/Yoshua_Bengio_2019_cropped.jpg\" width=\"300\">\n",
    "\n",
    "Yoshua Bengio  \n",
    "1964-Now  \n",
    "- Université de Montréal and scientific director of the Montreal Institute for Learning Algorithms (**MILA**).\n",
    "- Received the 2018 ACM A.M. **Turing Award**\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advantages of `RandomizedSearchCV`\n",
    "\n",
    "![](../img/randomsearch_bergstra.png)\n",
    "    \n",
    "Source: [Bergstra and Bengio, Random Search for Hyper-Parameter Optimization, JMLR 2012](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The yellow on the left shows how your scores are going to change when you vary the unimportant hyperparameter.\n",
    "- The green on the top shows how your scores are  going to change when you vary the important hyperparameter.\n",
    "- You don't know in advance which hyperparameters are important for your problem.\n",
    "- In the left figure, 6 of the 9 searches are useless because they are only varying the unimportant parameter.\n",
    "- In the right figure, all 9 searches are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for optimal parameters with successive halving¶\n",
    "\n",
    "Successive halving is an iterative selection process which **initially** evluates all candidates with **small amount of resources** and then **iteratively adds more and more resources.**\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_successive_halving_iterations_001.png\" width=\"700\">\n",
    "\n",
    "Learn more here: https://scikit-learn.org/stable/modules/grid_search.html#successive-halving-user-guide\n",
    "\n",
    "And be aware of different variations for GridSearch and RandomSearch:\n",
    "- [successive halving with grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html)\n",
    "- [random search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Optional / Bonus] Model-based HParam Optimization\n",
    "- Both `GridSearchCV` and `RandomizedSearchCV` do **each trial independently**\n",
    "  - Outcome of one trial **does not impact HParam decisions** of the next trials.\n",
    "- What if you could learn from your experience, e.g. learn that `max_depth=3` is bad?\n",
    "  - That could save time because you wouldn't try combinations involving `max_depth=3` in the future.\n",
    "- We can do this with `scikit-optimize`, which is a completely different package from `scikit-learn`\n",
    "- It uses a technique called \"model-based optimization\" and we'll specifically use \"**Bayesian optimization**\".\n",
    "  - In short, it uses machine learning to predict what hyperparameters will be good.\n",
    "  - **Machine learning on machine learning!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "Model-based HParam optimization is like **two layers** of Machine Learning\n",
    "<br><br>\n",
    "\n",
    "<img src=\"https://github.com/fmfn/BayesianOptimization/raw/master/examples/bo_example.png\" width=\"1000\">\n",
    "\n",
    "img source: https://github.com/fmfn/BayesianOptimization\n",
    "<br><br><br><br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- This is an active research area and there are sophisticated packages for this.\n",
    "\n",
    "Here are some examples \n",
    "- [hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn)\n",
    "- [auto-sklearn](https://github.com/automl/auto-sklearn)\n",
    "- [SigOptSearchCV](https://sigopt.com/docs/overview/scikit_learn)\n",
    "- [TPOT](https://github.com/rhiever/tpot)\n",
    "- [hyperopt](https://github.com/hyperopt/hyperopt)\n",
    "- [hyperband](https://github.com/zygmuntz/hyperband)\n",
    "- [SMAC](http://www.cs.ubc.ca/labs/beta/Projects/SMAC/)\n",
    "- [MOE](https://github.com/Yelp/MOE)\n",
    "- [pybo](https://github.com/mwhoffman/pybo)\n",
    "- [spearmint](https://github.com/HIPS/Spearmint)\n",
    "- [BayesOpt](https://github.com/rmcantin/bayesopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## (iClicker) Exercise 8.1 \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "- (A) If you get best results at the **edges of your parameter grid**, it might be a good idea to adjust the range of values in your parameter grid.   \n",
    "- (B) Grid search is **guaranteed** to find best hyperparameters values. \n",
    "- (C) In general, it is possible to get **different** hyperparameters in **different runs** of `RandomizedSearchCV`.\n",
    "\n",
    "Answer = ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (iClicker) Exercise 8.2 \n",
    "\n",
    "Suppose you have 10 hyperparameters, each with 4 possible values.   \n",
    "If you run `GridSearchCV` with this parameter grid,  \n",
    "how many cross-validation experiments will be carried out? \n",
    "\n",
    "Answer: ????\n",
    "\n",
    "## (iClicker) Exercise 8.3\n",
    "\n",
    "Suppose you have 10 hyperparameters, each with 4 possible values.   \n",
    "If you run `RandomizedSearchCV` with this parameter grid with `n_iter=20`,  \n",
    "how many cross-validation experiments will be carried out? \n",
    "\n",
    "Answer: ?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization bias/Overfitting of the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting of the validation error \n",
    "\n",
    "- **Why do we need to evaluate the model on the test set in the end?**\n",
    "  - Why do we have the test Set?\n",
    "  - Why isn't Validation Set enough?\n",
    "- Why not just use cross-validation on the whole dataset? \n",
    "- While carrying out hyperparameter optimization, we usually try over many possibilities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "If your **validation set is hit too many times**, we suffer from **optimization bias** or **overfitting the validation set**. \n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization bias of parameter learning (overfitting on train set)\n",
    "- Overfitting of the training error\n",
    "- An example: \n",
    "    - During training, we could search over tons of different decision trees.\n",
    "    - So we can get \"lucky\" and find a tree with low training error by chance.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization bias of hyper-parameter learning (overfitting on validation set)\n",
    "\n",
    "- Overfitting of the validation error\n",
    "- An example: \n",
    "    - Here, we might optimize the validation error over 1000 values of `max_depth`.\n",
    "    - One of the 1000 trees might have low validation error by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-------------\n",
    "**[Read at home; it's a fun experiment]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### (Optional) Example 1: Optimization bias\n",
    "\n",
    "Consider a multiple-choice (a,b,c,d) \"test\" with **10 questions**:\n",
    "- If you choose answers randomly, expected grade is 25% (no bias).\n",
    "- If you retry the same test twice, randomly, and pick the best, expected grade is 33%.\n",
    "    - Optimization bias of ~8%.\n",
    "- If you take the best among 10 random tests, expected grade is ~47%.\n",
    "- If you take the best among 100, expected grade is ~62%.\n",
    "- If you take the best among 1000, expected grade is ~73%.\n",
    "- If you take the best among 10000, expected grade is ~82%.\n",
    "    - You have so many \"chances\" that you expect to do well.\n",
    "    \n",
    "**But on new questions the \"random choice\" accuracy is still 25%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) Code attribution: Rodolfo Lourenzutti\n",
    "number_tests = [1, 2, 10, 100, 1000, 10000]\n",
    "for ntests in number_tests:\n",
    "    y = np.zeros(10000)\n",
    "    for i in range(10000):\n",
    "        y[i] = np.max(np.random.binomial(10.0, 0.25, ntests))\n",
    "    print(\n",
    "        \"The expected grade among the best of %d tests is : %0.2f\"\n",
    "        % (ntests, np.mean(y) / 10.0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### (Optional) Example 2: Optimization bias \n",
    "\n",
    "- If we instead used a **100-question** test then:\n",
    "    - Expected grade from best over 1 randomly-filled test is 25%.\n",
    "    - Expected grade from best over 2 randomly-filled test is ~27%.\n",
    "    - Expected grade from best over 10 randomly-filled test is ~32%.\n",
    "    - Expected grade from best over 100 randomly-filled test is ~36%.\n",
    "    - Expected grade from best over 1000 randomly-filled test is ~40%.\n",
    "    - Expected grade from best over 10000 randomly-filled test is ~43%.\n",
    "\n",
    "- The optimization bias **grows with the number of things we try**.\n",
    "    - “Complexity” of the set of models we search over.\n",
    "- But, optimization bias **shrinks quickly with the number of examples**.\n",
    "    - But it’s still non-zero and growing if you over-use your validation set!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) Code attribution: Rodolfo Lourenzutti\n",
    "number_tests = [1, 2, 10, 100, 1000, 10000]\n",
    "for ntests in number_tests:\n",
    "    y = np.zeros(10000)\n",
    "    for i in range(10000):\n",
    "        y[i] = np.max(np.random.binomial(100.0, 0.25, ntests))\n",
    "    print(\n",
    "        \"The expected grade among the best of %d tests is : %0.2f\"\n",
    "        % (ntests, np.mean(y) / 100.0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**[End of Read at home; it's a fun experiment]**\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization bias on the Spotify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tiny, X_test_big, y_train_tiny, y_test_big = train_test_split(\n",
    "    X_spotify, y_spotify, test_size=0.99, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tiny.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tiny.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# ----------- Attention Here -----------\n",
    "param_grid = {\n",
    "    \"svc__gamma\": 10.0 ** np.arange(-20, 10),\n",
    "    \"svc__C\": 10.0 ** np.arange(-20, 10),\n",
    "}\n",
    "# ----------------------------------------\n",
    "print(\"Grid size: %d\" % (np.prod(list(map(len, param_grid.values())))))\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Attention Here: n_iter=900 -----------\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe, param_distributions=param_grid, n_jobs=-1, n_iter=900, cv=5, random_state=123\n",
    ")\n",
    "random_search.fit(X_train_tiny, y_train_tiny);\n",
    "# ----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(random_search.cv_results_)[\n",
    "    [\n",
    "        \"mean_test_score\",\n",
    "        \"param_svc__gamma\",\n",
    "        \"param_svc__C\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_score\",\n",
    "    ]\n",
    "].set_index(\"rank_test_score\").sort_index().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the results: one might claim that we found a model that performs with 0.8 accuracy on our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Do we really believe that 0.80 is a good estimate of our test data?\n",
    "- Do we really believe that `gamma`=0.0 and C=1_000_000_000 are the best hyperparameters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's find out the test score with this best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "random_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`0.61` is much smaller than `0.8`; we have overfitted on the validation set!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The results above are overly optimistic. \n",
    "    - because our training data is very small and so our validation splits in cross validation would be small. \n",
    "    - because of the small dataset and the fact that **we hit the small validation set 900 times** and it's possible that we got lucky on the validation set! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- As we suspected, the best cross-validation score is not a good estimate of our test data; it is overly optimistic. \n",
    "- We can trust this test score because the test set is of good size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_big.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting of the validation data\n",
    "\n",
    "The following plot demonstrates what happens during overfitting of the validation data.  \n",
    "<center>\n",
    "<img src='../img/optimization-bias.png' width=\"600\">\n",
    "</center>\n",
    "\n",
    "\n",
    "[Source](https://amueller.github.io/COMS4995-s20/slides/aml-03-supervised-learning/#20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Thus, not only can we not trust the cv scores, we also cannot trust cv's ability to choose of the best hyperparameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why do we need a test set? \n",
    "- The frustrating part is that if **our dataset is small** then our **test set is also small**. \n",
    "- But we don't have a lot of better alternatives, unfortunately, if we have a small dataset. \n",
    "\n",
    "<br><br><br><br>\n",
    "**The larger your data, less chance of all overfitting.**  \n",
    "**The larger your data, more reliable are your reports (similar to statistical studies).**  \n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When test score is much lower than CV score\n",
    "- What to do if your test score is much lower than your cross-validation score:\n",
    "    - Try simpler models and use the test set a couple of times; it's not the end of the world.\n",
    "    - Communicate this clearly when you report the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Large datasets solve many of these problems\n",
    "- With infinite amounts of training data, overfitting would not be a problem and you could have your test score = your train score.\n",
    "    - Overfitting happens because you only see a bit of data and you learn patterns that are overly specific to your sample.\n",
    "    - If you saw \"all\" the data, then the notion of \"overly specific\" would not apply.\n",
    "- So, more data will make your test score better and robust. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you\n",
    "\n",
    "Attribution: From Mark Schmidt's notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Would you trust the model?  \n",
    "\n",
    "- You have a dataset and you give me half of it. I train a model using all the data you have given me and I tell you that the model accuracy is 0.99.   \n",
    "**Would it classify the rest of the data with similar accuracy?** \n",
    "\n",
    "1. Probably \n",
    "2. Probably not "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Would you trust the model?  \n",
    "\n",
    "- You have a dataset and you give me half of it. I build a model using 80% of the data given to me and report the accuracy of 0.95 on the remaining 20% of the data.    \n",
    "**Would it classify the rest of the data with similar accuracy?**\n",
    "\n",
    "1. Probably \n",
    "2. Probably not "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Would you trust the model?  \n",
    "\n",
    "- You have a dataset and you give me 1/10th of it. The dataset given to me is rather small and so I split it into 96% train and 4% validation split. I carry out hyperparameter optimization using a single 4% validation split and report validation accuracy of 0.97.    \n",
    "**Would it classify the rest of the data with similar accuracy?**\n",
    "\n",
    "1. Probably \n",
    "2. Probably not "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comments and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automated hyperparameter optimization\n",
    "\n",
    "- Advantages \n",
    "    - reduce human effort\n",
    "    - less prone to error and improve reproducibility\n",
    "    - data-driven approaches may be effective\n",
    "- Disadvantages\n",
    "    - may be hard to incorporate intuition\n",
    "    - be careful about overfitting on the validation set\n",
    "    - might require a lot of compute for a thorough HParam optimization if too many HParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, especially on typical datasets, we get back `scikit-learn`'s default hyperparameter values. This means that the defaults are well chosen by `scikit-learn` developers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The problem of finding the best values for the important hyperparameters is tricky because \n",
    "    - You may have a lot of them (e.g. deep learning). \n",
    "    - You may have multiple hyperparameters which may interact with each other in unexpected ways.    \n",
    "- The best settings depend on the specific data/problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional readings and resources\n",
    "\n",
    "- [Preventing \"overfitting\" of cross-validation data](http://www.robotics.stanford.edu/~ang/papers/cv-final.pdf) by Andrew Ng"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
