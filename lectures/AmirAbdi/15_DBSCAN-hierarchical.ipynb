{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Lecture 16: DBSCAN and Hierarchical Clustering\n",
    "------------\n",
    "UBC, 2022-23\n",
    "\n",
    "Instructor: Amir Abdi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, announcements, LOs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../code/.\")\n",
    "from plotting_functions import *\n",
    "from plotting_functions_unsup import *\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import seaborn as sns\n",
    "from ipywidgets import interactive\n",
    "from plotting_functions import *\n",
    "from plotting_functions_unsup import *\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster, linkage\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Announcements\n",
    "\n",
    "- HW6 is due Mar 15, 11:59pm\n",
    "- Our final exam is going to Thu Apr 20 2023 | 19:00 pm\n",
    "  - https://students.ubc.ca/enrolment/exams/exam-schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lecture, students are expected to be able to:\n",
    "\n",
    "- Identify **limitations of K-Means**.\n",
    "- Broadly explain how **DBSCAN** works.\n",
    "- Apply DBSCAN using `sklearn`. \n",
    "- Explain the effect of epsilon and minimum samples hyperparameters in DBSCAN.  \n",
    "- Explain the difference between core points, border points, and noise points in the context of DBSCAN. \n",
    "- Identify DBSCAN limitations.\n",
    "- Explain the idea of **hierarchical clustering**.\n",
    "- Visualize dendrograms using `scipy.cluster.hierarchy.dendrogram`.    \n",
    "- Explain the advantages and disadvantages of different clustering methods. \n",
    "- Apply clustering algorithms on image datasets and interpret clusters. \n",
    "- Recognize the impact of distance measure and representation in clustering methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap and motivation [[video](https://youtu.be/1ZwITQyWpkY)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means recap \n",
    "- We discussed K-Means clustering in the previous lecture. \n",
    "- Each cluster is represented by a center. \n",
    "- Given a new point, you can assign it to a cluster by computing the distances to all cluster centers and **picking the cluster with the smallest distance.**\n",
    "- It's a popular algorithm because \n",
    "    - It's easy to understand and implement.\n",
    "    - Runs relatively quickly and scales well to large datasets. \n",
    "    - `sklearn` has a more scalable variant called [`MiniBatchKMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html) which can handle very large datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means limitations\n",
    "\n",
    "- Relies on **random initialization** and so the outcome may change depending upon this initialization. \n",
    "- K-Means clustering **requires to specify the number of clusters** in advance.\n",
    "  - Very often you do not know the centers in advance. \n",
    "  - The elbow method or the silhouette method to find the optimal number of clusters are not always easy to interpret. \n",
    "- **All points will be assigned to a cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Variants of K-Means to help mitigate the randomization effect\n",
    "\n",
    "- **re-run the algorithm several times**. \n",
    "    - Check out `n_init` parameter of [sklearn's KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). \n",
    "    - **Best output will be chosen based on Inertia**\n",
    "- [K-Means++](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf)\n",
    "    - it picks the initial centroids which are far away from each other. \n",
    "    - In other words, K-Means++ gives more chance to select points that are far away from centroids already picked.    \n",
    "    - By default `sklearn` uses this strategy for initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "\n",
    "### K-Means limitations: Shape of K-Means clusters\n",
    "\n",
    "- K-Means partitions the space based on the closest mean. \n",
    "- Each cluster is defined solely by its center and so it can only capture relatively simple shapes. \n",
    "- So the boundaries between clusters are **linear**; It fails to identify **clusters with complex shapes**. \n",
    "[Source](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html).\n",
    "\n",
    "![](../img/kmeans_boundaries.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means: failure case 1\n",
    "\n",
    "- K-Means performs poorly if the clusters have more complex shapes (e.g., two moons data below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=42)\n",
    "plot_X_k_means(X, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means: failure case 2\n",
    "\n",
    "- It assumes that all directions are equally important for each cluster\n",
    "  - Fails to identify non-spherical clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some random cluster data\n",
    "X, y = make_blobs(random_state=170, n_samples=200)\n",
    "rng = np.random.RandomState(74)\n",
    "transformation = rng.normal(size=(2, 2))\n",
    "X = np.dot(X, transformation)\n",
    "plot_X_k_means(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means: failure case 3 \n",
    "\n",
    "- Again, K-Means is unable to capture complex cluster shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datasets.make_circles(n_samples=200, noise=0.06, factor=0.4)[0]\n",
    "plot_X_k_means(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Can we do better than this? \n",
    "- Another clustering algorithm called DBSCAN is able to tackle some of these cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DBSCAN [[video](https://youtu.be/T4NLsrUaRtg)]\n",
    "\n",
    "- **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DBSCAN introduction\n",
    "\n",
    "- DBSCAN is a **density-based** clustering algorithm. \n",
    "- Intuitively, it's based on the idea that clusters form dense regions in the data and so it works by identifying \"crowded\" regions in the feature space. \n",
    "- It can address some of the limitations of K-Means we saw above. \n",
    "    - It does not require the user to specify the number of clusters in advance. \n",
    "    - It can identify points that are not part of any clusters. \n",
    "    - It can capture clusters of complex shapes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try `sklearn`'s DBSCAN.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.08, random_state=42)\n",
    "\n",
    "# ------ new code ------------\n",
    "dbscan = DBSCAN(eps=0.2)\n",
    "dbscan.fit(X)\n",
    "# -----------------------\n",
    "\n",
    "plot_X_dbscan(X, dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- DBSCAN is able to capture half moons shape\n",
    "- **We don't not have to specify the number of clusters**. \n",
    "    - That said, it has two other non-trivial hyperparameters to tune. \n",
    "      - eps\n",
    "      - min_samples\n",
    "- There are two examples which have **not been assigned any label** (noise examples). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One more example of DBSCAN clusters capturing complex cluster shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datasets.make_circles(n_samples=200, noise=0.06, factor=0.4)[0]\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=3)\n",
    "dbscan.fit(X)\n",
    "plot_X_dbscan(X, dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does it work?\n",
    "\n",
    "- Iterative algorithm.  \n",
    "- Based on the idea that clusters form dense regions in the data. \n",
    "\n",
    "![](../img/DBSCAN_search.gif)\n",
    "\n",
    "<!-- <center>\n",
    "<img src=\"img/DBSCAN_search.gif\" alt=\"\" height=\"900\" width=\"900\"> \n",
    "</center>\n",
    " -->\n",
    "[Source](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two main hyperparameters\n",
    "- `eps`: determines what it means for points to be \"close\"\n",
    "- `min_samples`: determines the number of **neighboring points** we require to consider in order for a point to be part of a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Effect of `eps` hyperparameter\n",
    "\n",
    "- `eps`: determines what it means for points to be \"close\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(random_state=0, n_samples=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive(lambda eps=0.1: plot_dbscan_with_labels(X, eps, min_samples=3), eps=(0.1, 6, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(euclidean_distances(X, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Effect of `min_samples` hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive(\n",
    "    lambda min_samples=1: plot_dbscan_with_labels(X, eps=1.2, min_samples=min_samples),\n",
    "    min_samples=(1, 5, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More details on DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "There are three kinds of points.\n",
    "\n",
    "- **Core points** are the points that have at least `min_samples` points within a distance of `eps`\n",
    "\n",
    "- **Border points** are connected to a core point. They are within a distance of eps to core point but they have fewer than `min_samples` points within a distance of `eps`. \n",
    "\n",
    "- **Noise points** are the points which do not belong to any cluster. In other words, the points which have less than `min_samples` points within distance `eps` of the starting point are noise points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DBSCAN algorithm \n",
    "\n",
    "- Pick a point $p$ at random.\n",
    "- Check whether $p$ is a \"core\" point or not (if not, pick another point). \n",
    "  - You can check this by looking at the number of neighbours within **epsilon distance** if they have at least `min_samples` points in the neighbourhood\n",
    "- If $p$ is a core point, give it a colour (label). \n",
    "  - Spread the colour of $p$ to all of its neighbours.\n",
    "- Check if **any of the neighbours that received the colour is a core point**, \n",
    "  - if yes, spread the colour to its neighbors as well.\n",
    "- Once there are no more core points left to spread the colour, pick a new unlabeled point $p$ and repeat the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means vs. DBSCAN\n",
    "\n",
    "- In DBSCAN, you do not have to specify the number of clusters! \n",
    "    - Instead, you have to tune `eps` and `min_samples`. \n",
    "- Unlike K-Means, DBSCAN doesn't have to assign all points to clusters. \n",
    "    - The label is -1 if a point is unassigned.\n",
    "- In sklearn, there is no `predict` method for DBSCAN.\n",
    "    - DBSCAN only really clusters the points you have, not \"new\" or \"test\" points.\n",
    "    - (you can still do whatever you like with the clsuter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Illustration of hyperparameters `eps` and `min_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X)\n",
    "print(\"Cluster memberships:{}\".format(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Default values for hyperparameters don't work well on toy datasets. \n",
    "- All points have been marked as noise with the default values for `eps` and `min_samples`\n",
    "- Let's examine the **effect of changing these hyperparameters**. \n",
    "    - noise points: shown in white\n",
    "    - core points: bigger\n",
    "    - border points: smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework**: play with min_samples and eps in the above example and find a combination that results in 2 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_dbscan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Observations\n",
    "\n",
    "- Increasing `eps` ($\\uparrow$) (left to right in the plot above) means more points will be included in a cluster. \n",
    "    - `eps` = 1.0 either creates more clusters or more noise points, whereas eps=3.0 puts all points in one cluster with no noise points.  \n",
    "- Increasing `min_samples` ($\\uparrow$) (top to bottom in the plot above) means points in less dense regions will be labeled as noise. \n",
    "    - `min_samples=2`, for instance, has none or only a fewer noise points whereas `min_samples=5` has several noise points. \n",
    "- Here `min_samples` = 2.0 or 3.0 and `eps` = 1.5 is giving us the best results. \n",
    "- In general, it's not trivial to tune these hyperparameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question for you\n",
    "\n",
    "- Does the order that you pick the points matter in DBSCAN?\n",
    "<br><br><br><br>\n",
    "\n",
    "**Yes and No**. \n",
    "- Any of the cluster's core points is able to fully identify the cluster, with no randomness involved. \n",
    "- But, there is a possible conflict you might get is that if a border point is reachable from two clusters. In this case the assignment will be implementation dependent, but *usually* the border point will be assigned to the **first cluster that \"finds\" it**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating DBSCAN clusters \n",
    "We can use the silhouette method because it's not dependent on the idea of cluster centers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(random_state=100, centers=3, n_samples=300)\n",
    "dbscan = DBSCAN(eps=2, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "plot_X_dbscan(X, dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Yellowbrick is designed to work with K-Means and not with DBSCAN.\n",
    "# So it needs the number of clusters stored in n_clusters\n",
    "# It also needs `predict` method to be implemented.\n",
    "# So we are patching the object with the right attributes here so that we can use Yellowbrick to show Silhouette plots.\n",
    "# python is cool, eh?\n",
    "\n",
    "n_clusters = len(set(dbscan.labels_))\n",
    "dbscan.n_clusters = n_clusters\n",
    "dbscan.predict = lambda x: dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = SilhouetteVisualizer(dbscan, colors=\"yellowbrick\")\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: Pros and cons\n",
    "\n",
    "- Pros\n",
    "    - Can learn arbitrary cluster shapes\n",
    "    - Can detect outliers \n",
    "- Cons\n",
    "    - sklearn does not perform `predict` on new examples (but you can still use clusters for prediction if you like)\n",
    "    - Needs tuning of two non-obvious hyperparameters \n",
    "\n",
    "There is an improved version of DBSCAN called [`HDBSCAN` (hierarchical DBSCAN)](https://github.com/scikit-learn-contrib/hdbscan). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DBSCAN: failure cases\n",
    "\n",
    "- DBSCAN is able to capture complex clusters. But this doesn't mean that `DBSCAN` always works better. It has its own problems! \n",
    "- DBSCAN doesn't do well when we have **clusters with different densities**. \n",
    "    - You can play with the hyperparameters but it's not likely to help much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DBSCAN: failure cases\n",
    "\n",
    "- Let's consider this dataset with three clusters of varying densities.  \n",
    "- K-Means performs better compared to DBSCAN. But it has the benefit of knowing the value of $K$ in advance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_varied, y_varied = make_blobs(\n",
    "    n_samples=200, cluster_std=[1.0, 5.0, 1.0], random_state=10\n",
    ")\n",
    "plot_k_means_dbscan_comparison(X_varied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (iClicker) Exercise 16.1 \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "- (A) With tiny epsilon (`eps` in `sklearn`) and min samples=1 (`min_samples=1` in `sklearn`) we are likely to end up with each point in its own cluster. \n",
    "- (B) With a smaller value of `eps` and larger number for `min_samples` we are likely to end up with a one big cluster. \n",
    "- (C) K-Means is more susceptible to outliers compared to DBSCAN.  \n",
    "- (D) In DBSCAN to be part of a cluster, each point must have at least `min_samples` neighbours in a given radius (including itself). \n",
    "- (E) In DBSCAN, it is generally a good idea to run DBSCAN with a large number of different random orderings of training examples. \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical clustering [[video]()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation\n",
    "\n",
    "- Deciding how many clusters we want is a hard problem. \n",
    "- Often, it's useful to get a complete picture of similarity between points in our data before picking the number of clusters.  \n",
    "- Hierarchical clustering is helpful in these scenarios.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main idea\n",
    "\n",
    "1. Start with each point in its own cluster. \n",
    "2. Greedily merge most similar *clusters*. \n",
    "3. Repeat Step 2 until you obtain only one cluster ($n-1$ times)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing hierarchical clustering\n",
    "\n",
    "- Hierarchical clustering can be visualized using a tool called **a dendrogram**. \n",
    "- Unfortunately, `sklearn` cannot do it so we will use the package `scipy.cluster.hierarchy` for hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hierarchical clustering input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward\n",
    "X, y = make_blobs(random_state=0, n_samples=11)\n",
    "linkage_array = ward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_X_dendrogram(X, linkage_array, label_n_clusters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Every point goes through the journey of being on its own (its own cluster) and getting merged with some other bigger clusters. \n",
    "- The intermediate steps in the process provide us clustering with different number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dendrogram\n",
    "\n",
    "- Dendrogram is a tree-like plot. \n",
    "- On the x-axis we have data points. \n",
    "- On the y-axis we have **distances between clusters**. \n",
    "- We start with data points as leaves of the tree.  \n",
    "- New parent node is created for every two clusters that are joined. \n",
    "- The length of each branch shows how far the merged clusters go. \n",
    "    - In the dendrogram above going from three clusters to two clusters means merging far apart points because the branches between three cluster to two clusters are long. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to plot a dendrogram? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "ax = plt.gca()\n",
    "dendrogram(linkage_array, ax=ax)\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What do we mean by distance between clusters?\n",
    "\n",
    "- We know how to measure distance between points (e.g., using Euclidean distance). \n",
    "- How do we measure distances between clusters? \n",
    "- The **linkage criteria** determines how to find similarity between clusters:\n",
    "- Some example linkage criteria are: \n",
    "    - single linkage (minimum distance)\n",
    "    - average linkage (average distance)\n",
    "    - complete (or maximum) linkage (maximum distance)\n",
    "    - ward linkage    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_X_dendrogram(X, linkage_array, label_n_clusters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `single` linkage \n",
    "\n",
    "- **Merges two clusters that have the smallest *minimum* distance across all their points**\n",
    "- Let's use `scipy.cluster.hierarchy`'s `single` to get linkage information. \n",
    "- This method gives us matrix `Z` with the merging information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import (\n",
    "    average,\n",
    "    complete,\n",
    "    dendrogram,\n",
    "    fcluster,\n",
    "    single,\n",
    "    ward,\n",
    ")\n",
    "\n",
    "Z = single(X)\n",
    "columns = [\"c1\", \"c2\", \"distance(c1, c2)\", \"# observations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(Z, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The linkage returns a matrix `Z` of shape `n-1` (number of iterations) by 4:\n",
    "  - The rows represent iterations. \n",
    "  - First and second columns (c1 and c2 above): indexes of the clusters being merged.\n",
    "  - Third column (distance(c1, c2)): the distance between the clusters being merged.\n",
    "  - Fourth column (# observations): the number of examples in the newly formed cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating dendrogram with `single` linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram(Z)\n",
    "# Z is our single linkage matrix\n",
    "plt.title(\"Dendrogram (Single linkage)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `average` linkage \n",
    "\n",
    "- **Merges two clusters that have the smallest average distance between all their points.**\n",
    "- `scipy.cluster.hierarchy`'s `average` method gives us matrix `Z` with the merging information using average linkage.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = average(X)\n",
    "dendrogram(Z)\n",
    "# Dendrogram with average linkage\n",
    "plt.title(\"Dendrogram (Average linkage)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `complete` linkage\n",
    "\n",
    "- Merges two clusters that have the **smallest maximum distance between their points**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = complete(X)\n",
    "dendrogram(Z)\n",
    "# Dendrogram with complete linkage\n",
    "plt.title(\"Dendrogram (Complete linkage)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `ward` linkage\n",
    "\n",
    "- Picks two clusters to merge such that the **variance within all clusters increases the least**. \n",
    "- Often leads to equally sized clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = ward(X)\n",
    "dendrogram(Z)\n",
    "# Dendrogram with ward linkage\n",
    "plt.title(\"Dendrogram (Ward linkage)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hierarchical clustering on UN Subvotes dataset\n",
    "\n",
    "- Let's use a dataset of votes on UN resolutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "votes_df = pd.read_csv(\"../data/subvotes.csv\")\n",
    "votes = votes_df.pivot(index=\"country\", columns=\"rcid\")\n",
    "votes = votes[np.sum(np.isnan(votes), axis=1) < 1]\n",
    "print(votes.shape)\n",
    "votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have 17 countries and 368 votes. \n",
    "- Let's cluster countries based on how they vote. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We'll use [hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) here because we are interested in knowing whether the countries agreed or disagreed on resolutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_dendrogram(method=\"average\", metric=\"hamming\", w=22, h=12):\n",
    "    Z = linkage(votes, method=method, metric=metric)\n",
    "    fig, ax = plt.subplots(figsize=(w, h))\n",
    "    dendrogram(Z, labels=votes.index, ax=ax)\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylabel(\"Distance\", fontsize=w)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=80, fontsize=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_dendrogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) Truncation\n",
    "- If you want to truncate the tree in the dendrogram (specially when you have a big $n$) you can use the `truncate_mode`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Z = linkage(votes, method=\"average\", metric=\"hamming\")\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "dendrogram(Z, p=6, truncate_mode=\"lastp\", ax=ax, labels=votes.index);\n",
    "# p is the number of leaves when truncate mode is \"lastp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Alternatively, you can truncate the tree down to $x$ levels from the single cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "dendrogram(Z, p=6, truncate_mode=\"level\", ax=ax, labels=votes.index);\n",
    "# p is the max depth of the tree when truncate_mode is \"level\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our toy dataset to understand truncation better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "dendrogram(linkage_array, ax=ax)\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram(Z, p=3, truncate_mode=\"level\");\n",
    "# p is the max depth of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram(Z, p=4, truncate_mode=\"lastp\");\n",
    "# p is the number of leaf nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Flat cluster \n",
    "\n",
    "- To bring the clustering to a \"flat\" format, we can use `fcluster`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "cluster_labels = fcluster(Z, 6, criterion=\"maxclust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cluster_labels, votes.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this better, let's try it out on our toy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "dendrogram(linkage_array, ax=ax)\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = fcluster(linkage_array, 3, criterion=\"maxclust\")\n",
    "pd.DataFrame(cluster_labels, columns=[\"Cluster\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: If we increase number of clusters to 4, what are the members of the newly formed 4th cluster?\n",
    "\n",
    "Answer: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (iClicker) Exercise 16.2\n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "- (A) In hierarchical clustering we do not have to worry about initialization. \n",
    "- (B) Hierarchical clustering can only be applied to smaller datasets because dendrograms are hard to visualize for large datasets.\n",
    "- (C) In all the three clustering methods we saw (K-Means, DBSCAN, hierarchical clustering), there is a way to decide the granularity of clustering (i.e., how many clusters to pick). \n",
    "- (D) To get robust clustering we can naively ensemble cluster labels (e.g., pick the most popular label) produced by different clustering methods. \n",
    "- (E) If you have a high Silhouette score and very clean and robust clusters, it means that the algorithm has captured the semantic meaning in the data of our interest.   \n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applying clustering on face images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We'll be working with `sklearn`'s [Labeled Faces in the Wild dataset](https://scikit-learn.org/0.16/datasets/labeled_faces.html). \n",
    "- The dataset has images of celebrities from the early 2000s downloaded from the internet. \n",
    "\n",
    "> Credit: This example is based on the example from [here](https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch03.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5), subplot_kw={\"xticks\": (), \"yticks\": ()})\n",
    "for target, image, ax in zip(people.target, people.images, axes.ravel()):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(people.target_names[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "image_shape = people.images[0].shape\n",
    "print(\"people.images.shape: {}\".format(people.images.shape))\n",
    "print(\"Number of classes: {}\".format(len(people.target_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3,023 images stored as arrays of 5655 pixels (87 by 65), of 62 different people:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "counts = np.bincount(people.target)  # count how often each target appears\n",
    "df = pd.DataFrame(counts, columns=[\"count\"], index=people.target_names)\n",
    "df.sort_values(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's make the data less skewed by taking only 20 images of the each person. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(people.target.shape, dtype=bool)\n",
    "for target in np.unique(people.target):\n",
    "    mask[np.where(people.target == target)[0][:20]] = 1\n",
    "\n",
    "X_people = people.data[mask]\n",
    "y_people = people.target[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_people.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br><br>\n",
    "\n",
    "----------------\n",
    "\n",
    "**[Study on your own]**\n",
    "\n",
    "### Representation of images (Optional)\n",
    "\n",
    "- Representation of input is very important when you cluster examples. \n",
    "- In this example, we'll use **Principal Component Analysis (PCA) representation** (briefly talked about last session; not covered in this course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100, whiten=True, random_state=0)\n",
    "X_pca = pca.fit_transform(X_people)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering faces with K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll cluster the images with this new representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=10, random_state=10)\n",
    "km.fit(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the sizes of the clusters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_km = km.fit_predict(X_pca)\n",
    "print(\"Cluster sizes k-means: {}\".format(np.bincount(labels_km)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine cluster centers. Are they going to be real images from the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_images(km, X_people, y_people, target_names, X_pca=None, pca=None, cluster=0):\n",
    "    image_shape = (87, 65)\n",
    "    fig, axes = plt.subplots(1, 6, subplot_kw={'xticks': (), 'yticks': ()},\n",
    "                             figsize=(10, 10), gridspec_kw={\"hspace\": .3})\n",
    "    center = km.cluster_centers_[cluster]\n",
    "    \n",
    "    mask = km.labels_ == cluster\n",
    "    \n",
    "    if pca: \n",
    "        dists = np.sum((X_pca - center) ** 2, axis=1)\n",
    "        dists[~mask] = np.inf\n",
    "        inds = np.argsort(dists)[:5]\n",
    "        axes[0].imshow(pca.inverse_transform(center).reshape(image_shape), vmin=0, vmax=1)        \n",
    "    else: \n",
    "        dists = np.sum((X_people - center) ** 2, axis=1)\n",
    "        dists[~mask] = np.inf\n",
    "        inds = np.argsort(dists)[:5]\n",
    "        axes[0].imshow(center.reshape(image_shape), vmin=0, vmax=1)        \n",
    "        \n",
    "    axes[0].set_title('Cluster center %d'%(cluster))\n",
    "    i = 1\n",
    "    for image, label in zip(X_people[inds], y_people[inds]):\n",
    "        axes[i].imshow(image.reshape(image_shape), vmin=0, vmax=1)\n",
    "        axes[i].set_title(\"%s\" % (target_names[label].split()[-1]), fontdict={'fontsize': 9})    \n",
    "        i+=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_faces_cluster_centers(km, pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The centers found by K-Means are smooth versions of faces which makes sense. \n",
    "- Intuitively, they seem to capture some interesting characteristics of faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine images for different centers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cluster_images(\n",
    "    km, X_people, y_people, people.target_names, pca=pca, X_pca=X_pca, cluster=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cluster_images(\n",
    "    km, X_people, y_people, people.target_names, pca=pca, X_pca=X_pca, cluster=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering faces with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN()\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "print(\"Unique labels: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With default hyperparameters, we get all points as noise points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tuning `eps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.target_names[y_people].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine at distances between images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "dists = euclidean_distances(X_pca)\n",
    "np.fill_diagonal(dists, np.inf)\n",
    "\n",
    "dist_df = pd.DataFrame(\n",
    "    dists, index=people.target_names[y_people], columns=people.target_names[y_people]\n",
    ")\n",
    "\n",
    "dist_df.iloc[10:20, 10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [6, 7, 8, 9, 10, 11, 12, 14]:\n",
    "    print(\"\\neps={}\".format(eps))\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=3)\n",
    "    labels = dbscan.fit_predict(X_pca)\n",
    "    print(\"Number of clusters: {}\".format(len(np.unique(labels)) - 1))\n",
    "    print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For lower `eps` all images are labeled as noise.\n",
    "- For `eps=7` we get many noise points and a small cluster.\n",
    "- For `eps=8` and `eps=9` we get many noise points but we also get one large cluster and a one smaller cluster.\n",
    "- Starting `eps=10` we get one big cluster and noise points. \n",
    "- There is never more than one large cluster suggesting that all the images are more or less equally similar/dissimilar to the rest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Noise images identified by DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=14, min_samples=3)\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "print(\"Unique labels: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print_dbscan_noise_images(X_people, y_people, dbscan, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can guess why these images are noise images. There are odd angles, cropping, sun glasses, hands near faces etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine DBSCAN clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(min_samples=3, eps=8)\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "print(\"Number of clusters: {}\".format(len(np.unique(labels))))\n",
    "print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Some clusters correspond to people with distinct faces and facial expressions. \n",
    "- It's also capturing orientation of the face.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print_dbscan_clusters(X_people, y_people, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Clustering faces with hierarchical clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the dendrogram.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = ward(X_pca)\n",
    "plt.figure(figsize=(20, 15))\n",
    "dendrogram(Z, p=7, truncate_mode=\"level\", no_labels=True)\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = fcluster(Z, 40, criterion=\"maxclust\")  # let's get flat clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_functions_unsup import print_hierarchical_clusters\n",
    "hand_picked_clusters = [2, 3, 6, 29, 30, 36, 38]\n",
    "print_hierarchical_clusters(\n",
    "    X_people, y_people, people.target_names, cluster_labels, hand_picked_clusters\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[End of study on your own]\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final comments, summary, and reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Take-home message\n",
    "\n",
    "- We saw three methods for clustering: K-Means, DBSCAN, and hierarchical clustering. \n",
    "- There are many more clustering algorithms out there which we didn't talk about. For example see [this overview of clustering methods](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods). \n",
    "- Two important aspects of clustering\n",
    "    - Choice of **distance metric**\n",
    "    - Data representation\n",
    "- Choosing the appropriate hyper-parameters (number of clusters, epsilon, etc.) for a given problem is quite hard. \n",
    "- A lot of manual interpretation is involved in clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A few comments on clustering evaluation\n",
    "\n",
    "- If you know the ground truth, you can use metrics such as [adjusted random score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) or [normalized mutual information score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html). \n",
    "- We can't use accuracy scores. \n",
    "    - Because the labels themselves are meaningless in clustering.  \n",
    "- Usually labels are not available, and if it is available we would probably go with **supervised models**.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- The silhouette score works for different clustering methods and it can give us some intuition about the quality of clusters. But it's not very interpretable on real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A couple of ways to evaluate clustering: \n",
    "- Using *robustness-based* clustering metrics\n",
    "- The idea is to run a clustering algorithm (or a number of clustering algorithms) after **adding some noise to the data** or **using different parameter settings** and comparing outcomes. \n",
    "- If many models, perturbations, and parameters are giving the same result, the clustering is likely to be trustworthy.  \n",
    "\n",
    "\n",
    "**Even though all clustering models give similar results, the clusters might not capture the aspect you are interested in.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources \n",
    "\n",
    "- Check out this nice comparison of [sklearn clustering algorithms](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods).\n",
    "- [DBSCAN Visualization](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)\n",
    "- [Clustering with Scikit with GIFs](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
