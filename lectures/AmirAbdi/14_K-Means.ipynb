{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lecture 14: K-Means Clustering \n",
    "\n",
    "UBC 2022-23 W2\n",
    "\n",
    "Instructor: Amir Abdi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Announcements\n",
    "\n",
    "- HW6 is due Mar 15, 11:59pm\n",
    "- Hope you all learned something new in the midterm, btw, grades are out :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, announcements, LOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install the following package in the course environment.\n",
    "```\n",
    "conda install -c districtdatalabs yellowbrick\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c districtdatalabs yellowbrick -y\n",
    "\n",
    "# If conda failed to install yellowbrick because of version conflict, try pip\n",
    "# !pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../code/.\")\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import seaborn as sns\n",
    "from plotting_functions import *\n",
    "from plotting_functions_unsup import *\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning outcomes <a name=\"lo\"></a>\n",
    "\n",
    "From this lecture, students are expected to be able to:\n",
    "\n",
    "- Explain the **unsupervised** paradigm. \n",
    "- Explain the motivation and potential applications of **clustering**. \n",
    "- Define the clustering problem. \n",
    "- Explain the **K-Means algorithm**.  \n",
    "  - Apply `sklearn`'s `KMeans` algorithm.  \n",
    "  - Point out pros and cons of K-Means and the difficulties associated with choosing the right number of clusters.   \n",
    "  - Interpret the clusters discovered by K-Means. \n",
    "- Create the **Elbow plot** and **Silhouette plots** for a given dataset. \n",
    "- Visualize clusters in low dimensional space. \n",
    "- Use clustering for customer segmentation problem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of machine learning\n",
    "\n",
    "Here are some typical learning problems. \n",
    "\n",
    "- **Supervised learning**\n",
    "    - Training a model from input data and and its corresponding targets (labels) to predict targets for new examples.     \n",
    "- Self-supervised learning (SSL)\n",
    "    - Similar to Supervised, but without any labels/targets, instead the input data (e.g. `X`) is the target.\n",
    "- **Unsupervised learning**\n",
    "    - Training a model to find patterns in a dataset, typically an unlabeled dataset\n",
    "- Hybrid of Unsurpervised and Supervised\n",
    "    - **SSL** training of Large Language Models (LLM) on general data followed by supervised training on downstream tasks\n",
    "- Reinforcement learning\n",
    "    - A family of algorithms for finding suitable actions to take in a given situation in order to maximize a reward. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised learning\n",
    "\n",
    "<img src=\"../img/sup-learning.png\" height=\"1000\" width=\"1000\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised learning\n",
    "\n",
    "- Training data consists of observations ($X$) without any corresponding targets.\n",
    "- Unsupervised learning could be used to group similar things together in $X$ or to find underlying structure in the data. \n",
    "\n",
    "<img src=\"../img/unsup-learning.png\" alt=\"\" height=\"900\" width=\"900\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Can we learn without targets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Yes, but the learning will be focused on finding the underlying structures of the inputs themselves (rather than finding the function $f$ between input and output like we did in supervised learning models). \n",
    "\n",
    "- Examples:\n",
    "    - **Clustering** (this course)\n",
    "    - Dimensionality reduction (not covered in this course; but, if you know about **Principal Component Analysis (PCA)**, you know enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Labeled vs. Unlabeled data\n",
    "- If you have access to labeled training data, you're in the \"supervised\" setting. \n",
    "- You know what to do in that case from the previous lectures\n",
    "- **The amount of unlabeled data out there is much more than labeled data**\n",
    "- Annotated data can become \"stale\" after a while in cases such as fraud detection. \n",
    "- Can you still make sense of the data even though you do not have the labels? \n",
    "  - Yes! At least to a certain extent! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Supervised vs unsupervised learning\n",
    "\n",
    "- In supervised learning, we are given features $X$ and target $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "<tr style=\"background-color:white;\">\n",
    "    <td>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <td colspan=\"2\" style=\"text-align:center;\"> <b>Dataset 1</b> </td>\n",
    "                <td></td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>$x_1$</td>\n",
    "                <td>$y$</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 101.0\n",
    "                <td> Sick\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 98.5 \n",
    "                <td> Not Sick\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 93.8 \n",
    "                <td> Sick\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 104.3\n",
    "                <td> Sick\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 98.6 \n",
    "                <td> Not Sick\n",
    "            </tr>\n",
    "        </table>\n",
    "    </td>\n",
    "    <td>\n",
    "       <table>\n",
    "            <tr>\n",
    "                <td colspan=\"3\" style=\"text-align:center;\"> <b>Dataset2</b> </td>\n",
    "                <td></td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>$x_1$</td>\n",
    "                <td>$x_2$</td>\n",
    "                <td>$y$</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> -2.68\n",
    "                <td> 0.32 \n",
    "                <td>class 1\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> -2.71\n",
    "                <td> -0.18\n",
    "                <td> class 1\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 1.28  \n",
    "                <td> 0.69    \n",
    "                <td> class 2\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 0.93  \n",
    "                <td> 0.32   \n",
    "                <td> class 2\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 1.39\n",
    "                <td> -0.28 \n",
    "                <td> class 3\n",
    "            </tr>\n",
    "        </table>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In unsupervised learning, we are only given features $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr style=\"background-color:white;\">\n",
    "    <td>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <td colspan=\"2\" style=\"text-align:center;\"> <b>Dataset 1</b> </td>\n",
    "                <td></td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>$x_1$</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 101.0\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 98.5 \n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 93.8 \n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 104.3\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 98.6 \n",
    "            </tr>\n",
    "        </table>\n",
    "    </td>\n",
    "    <td>\n",
    "       <table>\n",
    "            <tr>\n",
    "                <td colspan=\"3\" style=\"text-align:center;\"> <b>Dataset 2</b> </td>\n",
    "                <td></td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>$x_1$</td>\n",
    "                <td>$x_2$</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> -2.68\n",
    "                <td> 0.32 \n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> -2.71\n",
    "                <td> -0.18\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 1.28  \n",
    "                <td> 0.69    \n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 0.93  \n",
    "                <td> 0.32   \n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td> 1.39\n",
    "                <td> -0.28 \n",
    "            </tr>\n",
    "        </table>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering motivation [[video](https://youtu.be/caAuUAXwpb8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "- Suppose you are asked to categorize the items below. What groups would you identify? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../img/food-clustering-activity.png\" height=\"600\" width=\"600\"> \n",
    "\n",
    "Source: Images taken from different recipe sites on the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you want to build a machine learning model to cluster such images how would you represent such images? \n",
    "- Do you think there is one correct way to cluster these images? \n",
    "- Imagine that we also have ratings data of food items and users for a large number of users. Can you exploit the power of community to recommend certain food items to a given user they are likely to consume? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why clustering? \n",
    "\n",
    "- Most of the data out there is unlabeled.  \n",
    "- Getting labeled training data is often difficult, expensive, or simply impossible in some cases. \n",
    "- Can we extract some useful information from unlabeled data? \n",
    "- The most intuitive way is to group similar examples together to get some insight into the data even though we do not have the targets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering  \n",
    "\n",
    "**Clustering** is the task of partitioning the dataset into groups called clusters.\n",
    "\n",
    "The goal of clustering is to discover underlying groups in a given dataset such that:\n",
    "- **examples in the same group are as similar as possible**\n",
    "- **examples in different groups are as different as possible**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input and possible output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=10, centers=3, n_features=2, random_state=10)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], markers=\"o\", ax=axes[0])\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y, markers=\"o\", ax=axes[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Think of clustering as colouring the points (e.g., blue, red, green) such that points with the same color are close to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y, markers=\"o\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Usually the clusters are identified by a **cluster label**. \n",
    "- These labels are arbitrary, and **relabeling the points (label switching)** does not make a difference. \n",
    "- What we care about is which points have the same labels and which ones have different labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is there a notion of \"correct\" grouping?\n",
    "\n",
    "- Very often we do not know how many clusters are there in the data or if there are any clusters at all. In real-world data, clusters are rarely as clear as in our toy example above. \n",
    "- There is a notion of coherent and optimal (in some sense) clusters but there is no absolute truth here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 1\n",
    "Which of the following grouping of emoticons is the \"correct\" grouping?\n",
    "\n",
    "![](../img/emoticon_clustering_example.png)\n",
    "\n",
    "<!-- <img src=\"img/emoticon_clustering_example.png\" alt=\"\" height=\"800\" width=\"800\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Both seem reasonable! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Meaningful groups in clustering \n",
    "- In clustering, meaningful groups are dependent on the **application**.\n",
    "- It usually helps if we have some prior knowledge about the data and the problem.   \n",
    "- This makes it hard for us to objectively measure the quality of a clustering algorithm (or think about \"true\" clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common applications: Data exploration\n",
    "\n",
    "Although there is no notion of the \"right\" answer, we might still get something useful out of clustering. There are a number of common applications for clustering. \n",
    "\n",
    "- Summarize data \n",
    "- compress data\n",
    "- Partition the data into groups before further processing. \n",
    "- You could use it in supervised learning setting as well: \n",
    "  - A) Carry out clustering and examine performance of your model on individual clusters. If the performance is lower on a particular cluster, you could either try building a separate model for that cluster and improve the overall performance of your supervised model\n",
    "  - B) If you have few number of labeled samples, you can cluster the data, and use those few labeled samples and assume the same label for those within the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common applications: Customer segmentation\n",
    "\n",
    "- Understand landscape of the market in businesses and craft targeted business or marketing strategies tailored for each group.\n",
    "\n",
    "<!-- ![](../img/customer-segmentation.png) -->\n",
    "\n",
    "<img src=\"../img/customer-segmentation.png\" alt=\"\" height=\"600\" width=\"600\"> \n",
    "\n",
    "[source](https://www.youtube.com/watch?v=zPJtDohab-g&t=134s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Document clustering\n",
    "\n",
    "Grouping articles on different topics from different news sources. For example, [Google News](https://news.google.com). \n",
    "\n",
    "![](../img/google_news.png)\n",
    "\n",
    "<!-- <img src=\"img/google_news.png\" alt=\"\" height=\"1200\" width=\"1200\">  -->\n",
    "    \n",
    "**You'll be working on document clustering in the lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Similarity and distances\n",
    "\n",
    "- Clustering is based on the **notion of similarity** or **distances** between points. \n",
    "- How do we determine similarity between points in a multi-dimensional space?\n",
    "- Can we use something like $k$-neighbours for similarity? \n",
    "  - Yes! That's a good start!  \n",
    "  - With $k$-neighbours we used Euclidean distances to find nearby points. \n",
    "  - We can use the same idea for clustering! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means clustering algorithm [[video](https://youtu.be/s6AvSZ1_l7I)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means clustering \n",
    "\n",
    "One of the most commonly used clustering algorithm. \n",
    "\n",
    "**Input**\n",
    "- `X` $\\rightarrow$ a set of data points  \n",
    "- `K` (or $k$ or `n_clusters`) $\\rightarrow$ **number of clusters** $\\rightarrow$ Hyper-parameter\n",
    "\n",
    "**Output**\n",
    "- `K` clusters (groups) of the data points \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means using `sklearn`\n",
    "- Before understanding the algorithm, let's try it with `sklearn`. \n",
    "- Consider the toy dataset above. \n",
    "- For this toy dataset, the three clusters are pretty clear.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=10, centers=3, n_features=2, random_state=10)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], markers=\"o\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "toy_df = pd.DataFrame(data=X, columns=[\"feat1\", \"feat2\"])\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `KMeans` `fit`\n",
    "Let's try `sklearn`'s `KMeans` algorithm on this dataset.\n",
    "- We need to decide how many clusters we want. Here we are passing 3. \n",
    "- We are only passing `X` because this is unsupervised learning; we do not have labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# -------- New code -----------------\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are only passing X because this is unsupervised learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `predict` of `KMeans`\n",
    "\n",
    "- The output of `KMeans` is $K$ clusters (groups) of the data points. \n",
    "- Calling `predict` on the **same train data** will give us the cluster assignment for each data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df_cl = toy_df.copy()\n",
    "toy_df_cl[\"cluster\"] = kmeans.predict(toy_df)\n",
    "toy_df_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cluster centers in  K-Means\n",
    "\n",
    "- In K-Means each cluster is represented by its **cluster center**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **3 cluster centers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers=\"o\")\n",
    "plt.legend()\n",
    "\n",
    "# ---- visualize cluster centers ----------\n",
    "mglearn.discrete_scatter(\n",
    "    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2], markers=\"*\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means predictions on new examples \n",
    "- We can also use `predict` on unseen examples!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_examples = np.array([[-1, -5], [2, 5.0]])\n",
    "new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.predict(new_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers=\"o\")\n",
    "plt.legend()\n",
    "mglearn.discrete_scatter(\n",
    "    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2], markers=\"*\"\n",
    ");\n",
    "\n",
    "# ----- plotting new examples --------\n",
    "mglearn.discrete_scatter(new_examples[:, 0], new_examples[:, 1], markers=\"^\", s=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means algorithm: How does it work?\n",
    "\n",
    "- Represent each cluster by its cluster center and assign a cluster membership to each data point. \n",
    "\n",
    "**Chicken-and-egg problem!**\n",
    "\n",
    "- If we knew cluster centers, we can simply assign each point to its **nearest center**.\n",
    "- Similarly, **if we knew assignments, we can calculate cluster centers**.  \n",
    "- But we do not know either 😟. \n",
    "\n",
    "<br><br><br><br><br>\n",
    "**Expectation Maximization** is a family of **iterative algorithms** in Computer Science, and K-Means is a special form of that.\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means clustering algorithm\n",
    "\n",
    "**Input**: Data points X and the number of clusters K\n",
    "\n",
    "**Initialization**: K **random initial centers** for the clusters\n",
    "\n",
    "**Iterative process**:\n",
    "\n",
    "repeat \n",
    "- Assign each example to the closest center.\n",
    "- Estimate new centers as _average_ of observations in a cluster.\n",
    "\n",
    "until **centers stop changing** or **maximum iterations have reached**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "Try this **Interactive Demo** to learn about K-Means: http://alekseynp.com/viz/k-means.html\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "-----------\n",
    "\n",
    "[Review these cells on your own]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-interactive Demo of K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's execute K-Means algorithm on our toy example. \n",
    "\n",
    "**Input**\n",
    "- The data points `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = toy_df.shape[0]\n",
    "print(\"Number of examples: \", n_examples)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let K (number of clusters) be 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Initialization \n",
    "\n",
    "- Random initialization for K initial centers of the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=14)\n",
    "centers_idx = np.random.choice(range(0, n_examples), size=k)\n",
    "centers_df = toy_df.iloc[centers_idx]\n",
    "centers = X[centers_idx]\n",
    "colours = [\"black\", \"blue\", \"red\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], marker=\"o\")\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c=colours, marker=\"*\", s=200)\n",
    "plt.title(\"Initial centers\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iterative process\n",
    "\n",
    "repeat \n",
    "\n",
    "- Assign each example to the closest center. (`update_Z`)\n",
    "- Estimate new centers as _average_ of observations in a cluster. (`update_centers`)\n",
    "\n",
    "until **centers stop changing** or **maximum iterations have reached**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to find closest centers? \n",
    "\n",
    "- First step in the iterative process is assigning examples to the closest center. \n",
    "- Let's consider distance of an example to all centers and assign that example to the closest center.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_example_dist(toy_df, centers_df, 6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to find closest centers?\n",
    "\n",
    "- Similarly, we can make cluster assignments for all points by calculating distances of all examples to the centers and assigning it to the cluster with smallest distance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "\n",
    "def update_Z(X, centers):\n",
    "    \"\"\"\n",
    "    returns distances and updated cluster assignments\n",
    "    \"\"\"\n",
    "    dist = euclidean_distances(X, centers)\n",
    "    return dist, np.argmin(dist, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to update centers?   \n",
    "\n",
    "- With the new cluster assignments for our data points, we update cluster centers. \n",
    "- New cluster centers are means of data points in each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centers(X, Z, old_centers, k):\n",
    "    \"\"\"\n",
    "    returns new centers\n",
    "    \"\"\"\n",
    "    new_centers = old_centers.copy()\n",
    "    for kk in range(k):\n",
    "        new_centers[kk] = np.mean(X[Z == kk], axis=0)\n",
    "    return new_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteration 1: Step 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign each example to the closest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, Z = update_Z(X, centers)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the current cluster assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_current_assinment(X, Z, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteration 1: Step 2 \n",
    "- Estimate new centers as _average_ of observations in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centers_it1 = update_centers(X, Z, centers, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how the centers moved in this iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_update_centroid(toy_df, 6, 4, new_centers_it1, centers, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteration 2: step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign each example to the closest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, Z = update_Z(X, new_centers_it1)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the current cluster assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_current_assinment(X, Z, new_centers_it1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteration 2: step 2\n",
    "\n",
    "- Estimate new centers as _average_ of observations in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centers_it2 = update_centers(X, Z, new_centers_it1, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how the centers moved in this iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_update_centroid(toy_df, 6, 4, new_centers_it2, new_centers_it1, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteration 3: step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign each example to the closest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, Z = update_Z(X, new_centers_it2)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the current cluster assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_current_assinment(X, Z, new_centers_it2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteration 3: step 2\n",
    "\n",
    "- Estimate new centers as _average_ of observations in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centers_it3 = update_centers(X, Z, new_centers_it2, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how the centers moved in this iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_update_centroid(toy_df, 6, 4, new_centers_it3, new_centers_it2, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteration 4: step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign each example to the closest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, Z = update_Z(X, new_centers_it3)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the current cluster assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_current_assinment(X, Z, new_centers_it3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteration 4: step 2\n",
    "\n",
    "- Estimate new centers as _average_ of observations in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centers_it4 = update_centers(X, Z, new_centers_it3, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The cluster centers are not moving anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_update_centroid(toy_df, 6, 4, new_centers_it4, new_centers_it3, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteration 5: step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign each example to the closest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, Z = update_Z(X, new_centers_it4)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the current cluster assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_current_assinment(X, Z, new_centers_it4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iteration 5: step 2\n",
    "\n",
    "- Estimate new centers as _average_ of observations in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centers_it5 = update_centers(X, Z, new_centers_it4, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The cluster centers are not moving anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_update_centroid(toy_df, 6, 4, new_centers_it5, new_centers_it4, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When to stop?\n",
    "\n",
    "- Seems like our centroids aren't changing anymore. \n",
    "- The algorithm has converged. So we stop! \n",
    "- K-Means always converges. It doesn't mean it finds the \"right\" clusters. It can converge to a sub-optimal solution.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_iterative(toy_df, 6, 4, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization is crucial. We'll talk about it in a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[End of non-interactive demo]\n",
    "\n",
    "-----------\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Example 2\n",
    "- Let's use the K-means on the iris dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## Iris dataset\n",
    "iris = datasets.load_iris()  # loading the iris dataset\n",
    "features = iris.data  # get the input data\n",
    "labels = iris.target_names[\n",
    "    iris.target\n",
    "]  # get the targets, in this case the types of the Iris flower\n",
    "\n",
    "iris_df = pd.DataFrame(features, columns=iris.feature_names)\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris has 4 features. We cannot visualize 4 features, so, we will use Principal Component Analysis (PCA) to reduce the dimensions to 2.\n",
    "\n",
    "\n",
    "**Wait, what is PCA?**\n",
    "- It reducs dimensionality (number of features) of the data in a way that explains the **most variance** in the data.\n",
    "- It is also used as an Exploratory Data Analysis (EDA) tool\n",
    "\n",
    "**How does PCA work?**\n",
    "- Learn the following if you are interested: Eigenvectors / Eigenvalues / Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: *We might come back to PCA once again in Recommendation Systems*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Reducing the dimensionality for plotting purposes\n",
    "\n",
    "# ------ new code ---------\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(features)\n",
    "# ----------------------\n",
    "data_iris = pd.DataFrame(pca.transform(features), columns=[\"$Z_1$\", \"$Z_2$\"])\n",
    "data_iris[\"target\"] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, distance (similarty) is Euclidean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot_unsup(data_iris, 20, 10, \"Iris dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Initialization\n",
    "\n",
    "- In this case, **we know that $k=3$**;\n",
    "  - If not, we might have aimed for 2 clusters\n",
    "- We are going to pick three points at random to use as initial centroids;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# RANDOM initialization\n",
    "k = 3\n",
    "centroids = np.random.choice(range(0, 150), size=k)\n",
    "centroids = data_iris.iloc[centroids, 0:2]\n",
    "plot_intial_center(data_iris, centroids, 22, 10, title=\"Iris dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Next, for each point in the dataset, we calculate the distance to each one of the centroids; \n",
    "\n",
    "\n",
    "- Let's do it for one point as example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot_example_dist(data_iris, centroids, 22, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dist = distance.cdist(data_iris.iloc[:, 0:2], centroids.iloc[:, 0:2])\n",
    "plot_first_assignment(data_iris, centroids, dist, 22, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot_iterative(data_iris, 22, 10, centroids.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we iterate until we get **converge**...\n",
    "\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### (Optional) Feature engineering using K-Means \n",
    "\n",
    "- K-Means could be used for feature engineering in supervised learning. \n",
    "- Examples: \n",
    "    - You could add a categorical feature: cluster membership\n",
    "    - You could add a continuous features: distance from each cluster center\n",
    "- See [this paper](http://ai.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing K [[video](https://youtu.be/M5ilrhcL0oY)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameter tuning for K\n",
    "\n",
    "- K-Means takes K (`n_clusters` in `sklearn`) as a hyperparameter. How do we pick K? \n",
    "\n",
    "- In supervised setting we carried out hyperparameter optimization based on cross-validation scores. \n",
    "\n",
    "- Since in unsupervised learning we do not have the target values, it becomes difficult to **objectively measure the effectiveness of the algorithms**.\n",
    "\n",
    "- There is no definitive approach.\n",
    "\n",
    "- However, some strategies might be useful to help you determine K. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Method 1: The Elbow method\n",
    "\n",
    "- This method looks at the sum of **intra-cluster distances**, which is also referred to as **inertia**. \n",
    "- The intra-cluster distance in our toy example above is given as   \n",
    "\n",
    "$$\\sum_{P_i \\in C_1}  distance(P_i, C_1)^2 + \\sum_{P_i \\in C_2}  distance(P_i, C_2)^2 + \\sum_{P_i \\in C_3} distance(P_i, C_3)^2$$\n",
    "\n",
    "Where \n",
    "- $C_1, C_2, C_3$ are centroids \n",
    "- $P_i$s are points within that cluster\n",
    "- $distance$ can be any distance, depending on the problem (e.g. Euclidean distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inertia \n",
    "\n",
    "You can access this intra-cluster distance or inertia as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=3, n_features=2, random_state=10)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], markers=\"o\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we iterate over different number of **K values** and calcualte the **inertia** for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "d = {\"K\": [], \"inertia\": []}\n",
    "for k in range(1, 100, 10):\n",
    "    model = KMeans(n_clusters=k).fit(X)\n",
    "    d[\"K\"].append(k)\n",
    "    d[\"inertia\"].append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The **inertia decreases as K increases**. \n",
    "- Question: Do we want inertia to be small or large? \n",
    "  - Answer: ?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The problem is that we can't just look for a $k$ that minimizes inertia because it decreases as $k$ increases.\n",
    "    - If I have number of clusters = number of examples, each example will have its own cluster and the intra-cluster distance will be 0.  \n",
    "- Instead we evaluate the trade-off between **two objectives**:\n",
    "  - \"small k\"\n",
    "  - \"small intra-cluster distances\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_elbow(w, h, inertia_values):\n",
    "    plt.figure(figsize=(w, h))\n",
    "    plt.axvline(x=3, linestyle=\"-.\", c=\"black\")\n",
    "    plt.plot(range(1, 10), inertia_values, \"-o\")\n",
    "    ax = plt.gca()\n",
    "    ax.tick_params(\"both\", labelsize=(w + h) / 2)\n",
    "    ax.set_xlabel(\"K\", fontsize=w)\n",
    "    ax.set_ylabel(\"Inertia\", fontsize=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "inertia_values = list()\n",
    "for k in range(1, 10):\n",
    "    inertia_values.append(KMeans(n_clusters=k).fit(toy_df).inertia_)\n",
    "plot_elbow(8, 6, inertia_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- From the above plot, we could argue that **3 clusters** are enough.\n",
    "- The inertia decreases when clusters are greater than 3. However it's not a big improvement and so we prefer K=3. \n",
    "- In this toy example, it's the plot is kind of clear and easy to interpret but it can be hard to interpret in real life examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br><br>\n",
    "\n",
    "There  package [`yellowbrick`](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html) which can be used to create these plots conveniently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check documentation of `KElbowVisualizer` here for KMeans: https://www.scikit-yb.org/en/latest/api/cluster/elbow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "model = KMeans()\n",
    "\n",
    "# ----- pass the model and HParams to KElbowVisualizer ------\n",
    "visualizer = KElbowVisualizer(model, k=(1, 10))\n",
    "\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method 2: The Silhouette method\n",
    "\n",
    "- Not dependent on the notion of cluster centers. \n",
    "- Calculated using the **mean intra-cluster distance** ($a$) and the **mean nearest-cluster distance** ($b$) for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Mean intra-cluster distance ($a$)\n",
    "\n",
    "- Suppose the green point below is our sample. \n",
    "- Average of the distances of the green point to the other points in the same cluster.\n",
    "  - These distances are represented by the black lines. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mean nearest-cluster distance ($b$)\n",
    "\n",
    "- Average of the distances of the green point to the blue points is smaller than the average of the distances of the green point to the red points. So the **nearest cluster** is the blue cluster. \n",
    "- So the mean nearest-cluster distance is the average of the distances of the green point to the blue points.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouette_dist(6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Silhouette distance for a sample \n",
    "\n",
    "- the difference between the **the average nearest-cluster distance** ($b$) and **average intra-cluster distance** ($a$) for each sample, normalized by the maximum value\n",
    "\n",
    "$$\\frac{b-a}{max(a,b)}$$\n",
    "\n",
    "\n",
    "In theory:\n",
    "- The **best** value is 1. \n",
    "- The worst value is -1 (samples have been assigned to wrong clusters).\n",
    "- Value near 0 means overlapping clusters. \n",
    "\n",
    "The overall **Silhouette score** is the average of the Silhouette scores for all samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using Silhouette scores to select the number of clusters\n",
    "\n",
    "- The plots below show the Silhouette scores for each sample in that cluster. \n",
    "- Higher values indicate well-separated clusters. \n",
    "- The size of the Silhouette shows the number of samples and hence shows imbalance of data points in clusters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(2, random_state=42)\n",
    "visualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show();\n",
    "# Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = KMeans(3, random_state=42)\n",
    "visualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show();\n",
    "# Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = KMeans(5, random_state=42)\n",
    "visualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show();\n",
    "# Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What to look for in these plots?\n",
    "\n",
    "- Unlike inertia, **larger values are better** because they indicate that the point is further away from neighbouring clusters.\n",
    "- The thickness of each silhouette indicates the cluster size.\n",
    "- The shape of each silhouette indicates the \"goodness\" for points in each cluster.\n",
    "  - **A slower dropoff (more rectangular)** indicates more points are \"happy\" in their cluster.\n",
    "  - **A fast drop** shows that some points are not very aligned within their cluster.\n",
    "  - **Negative values** are red flags and indicate something has gone terribly bad.\n",
    "- **The length (or area) of each silhouette indicates the goodness of each cluster.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = KMeans(3, random_state=42)\n",
    "visualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show();\n",
    "# Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comments on Silhouette scores \n",
    "- Unlike inertia, larger values are better because they indicate that the point is further away from neighbouring clusters.\n",
    "- Unlike inertia, **the overall silhouette score gets worse as you add more clusters because you end up being closer to neighbouring clusters.**\n",
    "- Thus, as with intertia, you will not see a \"peak\" value of this metric that indicates the best number of clusters.\n",
    "- We can visualize the silhouette score for each example individually in a silhouette plot (hence the name), see below.\n",
    "- We can apply Silhouette method to clustering methods other than K-Means. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iClicker) Exercise 14.1 \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "- (A) K-Means may converge to different solutions depending upon the initialization.\n",
    "- (B) K-means terminates when the number of clusters does not increase between iterations.\n",
    "- (C) $K$ in K-Means should always be $\\leq$ # of features.\n",
    "- (D) In K-Means, it makes sense to have $K$ $<$ # of examples. \n",
    "- (E) In K-Means, in some iterations some points may be left unassigned. \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iClicker) Exercise 14.2\n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "- (A) The preprocessing methods such as `StandardScaler` are unsupervised methods. \n",
    "- (B) If the centroid locations do not change between iterations, K-means terminates.\n",
    "- (C) If you train K-Means with `n_clusters= the number of examples`, the inertia value will be 0. \n",
    "- (D) Unlike the Elbow method, the Silhouette method is not dependent on the notion of cluster centers.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means case study: Customer segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is customer segmentation? \n",
    "\n",
    "- Understand landscape of the market in businesses and craft targeted business or marketing strategies tailored for each group.\n",
    "\n",
    "<img src=\"../img/customer-segmentation.png\" alt=\"\" height=\"600\" width=\"600\"> \n",
    "\n",
    "[source](https://www.youtube.com/watch?v=zPJtDohab-g&t=134s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [this interesting talk by Malcom Gladwell](https://www.ted.com/talks/malcolm_gladwell_on_spaghetti_sauce?language=en). Humans are diverse and there is no single spaghetti sauce that would make all of them happy! \n",
    "\n",
    "Often it's beneficial to businesses to explore the landscape of the market and tailor their services and products offered to each group. This is called **customer segmentation**. It's usually applied when the dataset contains some of the following features. \n",
    "\n",
    "- **Demographic information** such as gender, age, marital status, income, education, and occupation\n",
    "- **Geographical information** such as specific towns or counties or a customer's city, state, or even country of residence (in case of big global companies)\n",
    "- **Psychographics** such as social class, lifestyle, and personality traits\n",
    "- **Behavioral data** such as spending and consumption habits, product/service usage, and desired benefits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Business problem \n",
    "\n",
    "- Imagine that you are hired as a data scientist at a bank. They provide some data of their credit card customers to you. \n",
    "- Their goal is to develop customized marketing campaigns and they ask you to group customers based on the given information. \n",
    "- Now that you know about K-Means clustering, let's apply it to the dataset to group customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "- We will use the [Credit Card Dataset for clustering](https://www.kaggle.com/arjunbhasin2013/ccdata) from Kaggle.\n",
    "- Download the data and save the CSV under the `data` folder. \n",
    "- I encourage you to work through this case study on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "creditcard_df = pd.read_csv(\"../data/CC General.csv\")\n",
    "creditcard_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Information of the dataset \n",
    "\n",
    "We have behavioral data. \n",
    "\n",
    "- CUSTID: Identification of Credit Card holder\n",
    "- BALANCE: Balance amount left in customer's account to make purchases\n",
    "- BALANCE_FREQUENCY: How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n",
    "- PURCHASES: Amount of purchases made from account\n",
    "- ONEOFFPURCHASES: Maximum purchase amount done in one-go\n",
    "- INSTALLMENTS_PURCHASES: Amount of purchase done in installment\n",
    "- CASH_ADVANCE: Cash in advance given by the user\n",
    "- PURCHASES_FREQUENCY: How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n",
    "- ONEOFF_PURCHASES_FREQUENCY: How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n",
    "- PURCHASES_INSTALLMENTS_FREQUENCY: How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n",
    "- CASH_ADVANCE_FREQUENCY: How frequently the cash in advance being paid\n",
    "- CASH_ADVANCE_TRX: Number of Transactions made with \"Cash in Advance\"\n",
    "- PURCHASES_TRX: Number of purchase transactions made\n",
    "- CREDIT_LIMIT: Limit of Credit Card for user\n",
    "- PAYMENTS: Amount of Payment done by user\n",
    "- MINIMUM_PAYMENTS: Minimum amount of payments made by user\n",
    "- PRC_FULL_PAYMENT: Percent of full payment paid by user\n",
    "- TENURE: Tenure of credit card service for user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All numeric features\n",
    "- Some missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "creditcard_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Practice exercises for you\n",
    "\n",
    "1. What is the average `BALANCE` amount?\n",
    "2. How often the `BALANCE_FREQUENCY` is updated on average? \n",
    "3. Obtain the row the customer who made the maximum cash advance transaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Mini exercises for you (Answers)\n",
    "\n",
    "1. What is the average `BALANCE` amount? 1564.47\n",
    "2. How often the `BALANCE_FREQUENCY` is updated on average? 0.877 (pretty often) \n",
    "3. Obtain the row of the customer who made the maximum cash advance transaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3.\n",
    "max_cash_advance = creditcard_df[\"CASH_ADVANCE\"].max()\n",
    "creditcard_df[creditcard_df[\"CASH_ADVANCE\"] == max_cash_advance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine correlations between features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = creditcard_df.corr()\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature types and preprocessing \n",
    "\n",
    "Let's identify different feature types and transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = [\"CUST_ID\"]\n",
    "numeric_features = list(set(creditcard_df.columns) - set(drop_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What kind of preprocessing do we need to apply? \n",
    "- Do we need to scale the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_transformer = make_pipeline(SimpleImputer(), StandardScaler())\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features), (\"drop\", drop_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "transformed_df = pd.DataFrame(\n",
    "    data=preprocessor.fit_transform(creditcard_df), columns=numeric_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have transformed the data, we are ready to run K-Means to cluster credit card customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing `n_clusters`\n",
    "\n",
    "- There is no definitive method to find the optimal number of clusters. \n",
    "- Let's try different approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(random_state=42)\n",
    "visualizer = KElbowVisualizer(model, k=(1, 20))\n",
    "\n",
    "visualizer.fit(transformed_df)  # Fit the data to the visualizer\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The optimal number of clusters is not as clear as it was in our toy example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's examine Silhouette scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3, 6):\n",
    "    model = KMeans(k, random_state=42)\n",
    "    visualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\n",
    "    visualizer.fit(transformed_df)  # Fit the data to the visualizer\n",
    "    visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- I'm going to run `KMeans` with `n_clusters = 4`. \n",
    "- You can try out `n_clusters = 5` and `n_clusters = 6` on your own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing clusters \n",
    "\n",
    "- Can we visualize the clusters? \n",
    "- We have a high dimensional data and we need to reduce the dimensionality in order to visualize it. \n",
    "- Let's reduce the dimensionality using a technique called [UMAP](https://umap-learn.readthedocs.io/en/latest/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I forgot to put this package in the course environment file. So to run the code below, you'll have to install the `umap-learn` package in the course conda environment either with `conda` or `pip`, as described in the [documentation](https://umap-learn.readthedocs.io/en/latest/index.html). \n",
    "\n",
    "```\n",
    "> conda install -c conda-forge umap-learn\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```\n",
    "> pip install umap-learn\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_clusters(\n",
    "    data,\n",
    "    cluster_labels,\n",
    "    size=50,\n",
    "    n_neighbors=15,\n",
    "    title=\"UMAP visualization\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Carry out dimensionality reduction using UMAP and plot 2-dimensional clusters.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    data : numpy array\n",
    "        data as a numpy array\n",
    "    cluster_labels : list\n",
    "        cluster labels for each row in the dataset\n",
    "    size : int\n",
    "        size of points in the scatterplot\n",
    "    n_neighbors : int\n",
    "        n_neighbors hyperparameter of UMAP. See the documentation.\n",
    "    title : str\n",
    "        title for the visualization plot\n",
    "\n",
    "    Returns\n",
    "    -----------\n",
    "    None. Shows the clusters.\n",
    "    \"\"\"\n",
    "\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors)\n",
    "    Z = reducer.fit_transform(data)  # reduce dimensionality\n",
    "    umap_df = pd.DataFrame(data=Z, columns=[\"dim1\", \"dim2\"])\n",
    "    umap_df[\"cluster\"] = cluster_labels\n",
    "\n",
    "    labels = np.unique(umap_df[\"cluster\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.set_title(title)\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        umap_df[\"dim1\"],\n",
    "        umap_df[\"dim2\"],\n",
    "        c=umap_df[\"cluster\"],\n",
    "        cmap=\"tab20b\",\n",
    "        s=size,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=0.1,\n",
    "    )\n",
    "\n",
    "    legend = ax.legend(*scatter.legend_elements(), loc=\"best\", title=\"Clusters\")\n",
    "    ax.add_artist(legend)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for k in range(3, 7):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(transformed_df)\n",
    "    labels = kmeans.labels_\n",
    "    plot_umap_clusters(transformed_df, kmeans.labels_, title=f\"K-Means with k = {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The clusters above look reasonably well separated. \n",
    "- This might not always be the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cluster interpretation\n",
    "\n",
    "- Let's examine the cluster centers for k=4 and identify types of customers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasonable_k = 4\n",
    "kmeans = KMeans(n_clusters=reasonable_k, random_state=42)\n",
    "kmeans.fit(transformed_df)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = pd.DataFrame(\n",
    "    data=kmeans.cluster_centers_, columns=[transformed_df.columns]\n",
    ")\n",
    "cluster_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Recall that we have applied imputation and scaling on the dataset. \n",
    "- But we would be able to interpret these clusters better if the centers are in the original scale. \n",
    "- So let's apply **inverse transformations to get the cluster center values in the original scale**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    preprocessor.named_transformers_[\"pipeline\"]\n",
    "    .named_steps[\"standardscaler\"]\n",
    "    .inverse_transform(cluster_centers[numeric_features])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_cluster_centers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "org_cluster_centers = pd.DataFrame(data=data, columns=numeric_features)\n",
    "org_cluster_centers = org_cluster_centers.reindex(\n",
    "    sorted(org_cluster_centers.columns), axis=1\n",
    ")\n",
    "org_cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = {0: \"Transactors\", 1: \"Revolvers\", 2: \"Low activity\", 3: \"VIP/Prime\"}\n",
    "org_cluster_centers[\"cluster_labels\"] = list(cluster_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cols = [\n",
    "    \"cluster_labels\",\n",
    "    \"BALANCE\",\n",
    "    \"CREDIT_LIMIT\",\n",
    "    \"PRC_FULL_PAYMENT\",\n",
    "    \"PURCHASES_FREQUENCY\",\n",
    "    \"CASH_ADVANCE\",\n",
    "    \"CASH_ADVANCE_FREQUENCY\",\n",
    "    \"CASH_ADVANCE_TRX\",\n",
    "]\n",
    "org_cluster_centers[relevant_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "One way to interpret and label the clusters above is as follows. \n",
    "\n",
    "#### Transactors\n",
    "- Credit card users who pay off their balance every month with least amount of interest charges. \n",
    "- They are careful with their money. \n",
    "- They have lowest balance and cash advance\n",
    "\n",
    "#### Revolvers\n",
    "- Credit card users who pay off only part of their monthly balance. They use credit card as a loan.  \n",
    "- They have highest balance and cash advance, high cash advance frequency, low purchase frequency, high cash advance transactions, low percentage of full payment\n",
    "- Their credit limit is also high. (Lucrative group for banks 😟.)\n",
    "\n",
    "#### Low activity\n",
    "- There is not much activity in the account. It has low balance and not many purchases. \n",
    "- Credit card users who have low credit limit.\n",
    "\n",
    "#### VIP/Prime\n",
    "- Credit card users who have high credit limit. \n",
    "- They have high one-off purchases frequency, high number of purchase transactions. \n",
    "- They have high balance but they also have higher percentage of full payment, similar to transactors\n",
    "- Target for increase credit limit (and increase spending habits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More on interpretation of clusters\n",
    "\n",
    "- In real life, you'll look through all features in detail before assigning meaning to clusters. \n",
    "- This is not that easy, especially when you have a large number of features and clusters. \n",
    "- One way to approach this would be visualizing the distribution of feature values for each cluster. \n",
    "- Some domain knowledge would definitely help at this stage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_df['cluster'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the cluster assignment for the customer who made the maximum cash advance transaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_df[creditcard_df[\"CASH_ADVANCE\"] == max_cash_advance] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's this client's cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels[creditcard_df[creditcard_df[\"CASH_ADVANCE\"] == max_cash_advance]['cluster'].values[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_hists(df=creditcard_df, cols=[\"BALANCE\", \"CASH_ADVANCE\"]):\n",
    "    for i in cols:\n",
    "        plt.figure(figsize=(35, 5))\n",
    "        for j in range(4):\n",
    "            plt.subplot(1, 4, j + 1)\n",
    "            cluster = df[df[\"cluster\"] == j]\n",
    "            cluster[i].hist(bins=20)\n",
    "            plt.title(f\"{i}    \\nCluster: {cluster_labels[j]} \")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hists() # Examining clusters for two features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below to show histograms for all features. \n",
    "# cols = creditcard_df_cluster.columns.to_list()\n",
    "# cols.remove('CUST_ID')\n",
    "# cols.remove('cluster')\n",
    "# show_hists(creditcard_df_cluster, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice exercise for you\n",
    "- Try out different values for `n_clusters` in `KMeans` and examine the clusters. \n",
    "- If you are feeling adventurous, you may try customer segmentation on [All Lending Club loan data](https://www.kaggle.com/wordsforthewise/lending-club). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final comments, summary, and reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A comment on initialization\n",
    "\n",
    "- The initialization of K-Means is stochastic, can this affect the results?\n",
    "    - Answer: ???\n",
    "- Let's  look at an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=20, centers=3, n_features=2, random_state=10)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], markers=\"o\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "n_examples = X.shape[0]\n",
    "toy_df = pd.DataFrame(X, columns=[\"feat1\", \"feat2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Bad initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=10)\n",
    "centroids_idx = np.random.choice(range(0, n_examples), size=k)\n",
    "centroids = X[centroids_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iterative(toy_df, 6, 4, centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Better initialization \n",
    "The following initialization seems much better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=2)\n",
    "centroids_idx = np.random.choice(range(0, n_examples), size=k)\n",
    "centroids = X[centroids_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iterative(toy_df, 6, 4, centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What can we do about it?\n",
    "\n",
    "- One strategy is to **re-run the algorithm several times**. \n",
    "    - Check out `n_init` parameter of [`sklearn`'s `KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). \n",
    "    - **Best output will be chosen based on Inertia**\n",
    "- Is it possible to pick `K` in a smart way? \n",
    "    - Yes! We can use the so-called [K-Means++](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf).\n",
    "    - Intuitively, it picks the initial centroids which are far away from each other. \n",
    "    - In other words, K-Means++ gives more chance to select points that are far away from centroids already picked.    \n",
    "    - By default `sklearn` uses this strategy for initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### (Optional) Time complexity of K-Means\n",
    "\n",
    "- Naive implementation of K-Means requires you to compute the distances from all data points to all cluster centers. \n",
    "- So there are many distance calculations per iteration. \n",
    "- calculating assigning observations to centers is heavy: $\\mathcal{O(ndk)}$\n",
    "- updating centers is light(er): $\\mathcal{O(nd)}$\n",
    "\n",
    "where, \n",
    "\n",
    "- $n \\rightarrow$ number of examples\n",
    "- $d \\rightarrow$ number of features\n",
    "- $k \\rightarrow$ number of clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important points to remember\n",
    "\n",
    "- Clustering is a common unsupervised approach to identify underlying structure in data and grouping points based on similarity. \n",
    "- K-Means is a popular clustering algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important points to remember\n",
    "\n",
    "**K-Means**\n",
    "- It requires us to **specify the number of clusters** in advance. \n",
    "- Each example is assigned to one (and only one) cluster.\n",
    "- The labels provided by the algorithm have no actual meaning. \n",
    "- The centroids live in the same space as of the dataset but they are **not** actual data points, but instead are average points.\n",
    "- **It always converges to at least a local minima**. \n",
    "  - but its convergence to a global minima is not guaranteed\n",
    "- Convergence is dependent upon the initial centers and it may converge to a sub-optimal solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important points to remember\n",
    "\n",
    "- Two ways to provide insight into how many clusters are reasonable for the give problem are: **the Elbow method** and **the Silhouette method**.  \n",
    "- Some applications of K-Means clustering include data exploration, feature engineering, customer segmentation, and document clustering. \n",
    "- It takes fair amount of manual effort and domain knowledge to interpret clusters returned by K-Means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Resources \n",
    "- [\"Spaghetti Sauce\" talk by Malcom Gladwell](https://www.ted.com/talks/malcolm_gladwell_on_spaghetti_sauce?language=en)\n",
    "- [Visualizing-k-means-clustering](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) \n",
    "- [Visualizing K-Means algorithm with D3.js](http://tech.nitoyon.com/en/blog/2013/11/07/k-means/)\n",
    "- [Clustering with Scikit with GIFs](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)\n",
    "- [`sklearn` clustering documentation](https://scikit-learn.org/stable/modules/clustering.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
